###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env ######
# Database Configuration
MYSQL_ROOT_PASSWORD=secure_root_pw123
MYSQL_DATABASE=employees
MYSQL_USER=myuser
MYSQL_PASSWORD=userpass123
MYSQL_HOST=mysql-db
MYSQL_PORT=3306

# Monitor User Configuration
MYSQL_MONITOR_USER=monitor
MYSQL_MONITOR_PASSWORD=monitorpass123

# Performance Settings
BATCH_SIZE=1000
TOTAL_EMPLOYEES=10000

# API Configuration
API_PORT=3000

# Load Generator Configuration
K6_VUS=50
K6_DURATION=30m
NEW_RELIC_LICENSE_KEY=eu01xx0ce15fea0f04db4b9d9ecce89dFFFFNRAL
NEW_RELIC_APP_NAME=MySQL-Perf-Demo
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env.docker ######
# Database Configuration
MYSQL_HOST=mysql-db
MYSQL_PORT=3306
MYSQL_USER=myuser
MYSQL_PASSWORD=userpass123
MYSQL_DATABASE=employees
MYSQL_ROOT_PASSWORD=secure_root_pw123

# Monitor User Configuration
MYSQL_MONITOR_USER=monitor
MYSQL_MONITOR_PASSWORD=monitorpass123

# Performance Settings
BATCH_SIZE=1000
TOTAL_EMPLOYEES=10000

# API Configuration
API_PORT=3000
NODE_ENV=production

# Load Generator Configuration
K6_VUS=50
K6_DURATION=30m

# New Relic Configuration
NEW_RELIC_LICENSE_KEY=your_license_key
NEW_RELIC_APP_NAME=MySQL-Perf-Demo

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env.example ######
MYSQL_ROOT_PASSWORD=demo123
MYSQL_DATABASE=employees
MYSQL_USER=myuser
MYSQL_PASSWORD=demopass

# New Relic MySQL Monitor User
MYSQL_MONITOR_USER=newrelic
MYSQL_MONITOR_PASSWORD=nrpass123

# API Configuration
API_PORT=3000

# Data Generation
BATCH_SIZE=5000
TOTAL_EMPLOYEES=100000

# Performance Testing
ENABLE_SLOW_QUERIES=true
SLOW_QUERY_TIME=0.5

# Load Generator Configuration
K6_VUS=100
K6_DURATION=30m

# New Relic Configuration
NEW_RELIC_LICENSE_KEY=your_license_key
NEW_RELIC_APP_NAME=MySQL-Demo

# Environment
NODE_ENV=development

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env.local.template ######
# Database Configuration
MYSQL_HOST=host.docker.internal
MYSQL_PORT=3306
MYSQL_USER=local_user
MYSQL_PASSWORD=local_pass
MYSQL_DATABASE=employees
MYSQL_ROOT_PASSWORD=local_root_pw

# Monitor User Configuration
MYSQL_MONITOR_USER=monitor
MYSQL_MONITOR_PASSWORD=monitor_pass

# Performance Settings
BATCH_SIZE=1000
TOTAL_EMPLOYEES=5000

# API Configuration
API_PORT=3000
NODE_ENV=development

# Load Generator Configuration
K6_VUS=10
K6_DURATION=5m

# New Relic Configuration
NEW_RELIC_LICENSE_KEY=your_local_license_key
NEW_RELIC_APP_NAME=MySQL-Perf-Demo-Local

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.gitignore ######
# Environment files
*.env
!.env.example
!.env.docker
!.env.local.template
.env.local
.env.*.local
docker-compose.override.yml

# Existing entries remain unchanged
*.log
logs/
mysql_logs/
node_modules/
__pycache__/
*.pyc
.vscode/
.idea/
*.swp
*.swo
*~
mysql_data/
data/
.docker/
*.tmp
*.temp
.DS_Store

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\combined_content.txt ######
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env ######
# Database Configuration
MYSQL_ROOT_PASSWORD=secure_root_pw123
MYSQL_DATABASE=employees
MYSQL_USER=myuser
MYSQL_PASSWORD=userpass123
MYSQL_HOST=mysql-db
MYSQL_PORT=3306

# Monitor User Configuration
MYSQL_MONITOR_USER=monitor
MYSQL_MONITOR_PASSWORD=monitorpass123

# Performance Settings
BATCH_SIZE=1000
TOTAL_EMPLOYEES=10000

# API Configuration
API_PORT=3000

# Load Generator Configuration
K6_VUS=50
K6_DURATION=30m
NEW_RELIC_LICENSE_KEY=eu01xx0ce15fea0f04db4b9d9ecce89dFFFFNRAL
NEW_RELIC_APP_NAME=MySQL-Perf-Demo
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env.docker ######
# Database Configuration
MYSQL_HOST=mysql-db
MYSQL_PORT=3306
MYSQL_USER=myuser
MYSQL_PASSWORD=userpass123
MYSQL_DATABASE=employees
MYSQL_ROOT_PASSWORD=secure_root_pw123

# Monitor User Configuration
MYSQL_MONITOR_USER=monitor
MYSQL_MONITOR_PASSWORD=monitorpass123

# Performance Settings
BATCH_SIZE=1000
TOTAL_EMPLOYEES=10000

# API Configuration
API_PORT=3000
NODE_ENV=production

# Load Generator Configuration
K6_VUS=50
K6_DURATION=30m

# New Relic Configuration
NEW_RELIC_LICENSE_KEY=your_license_key
NEW_RELIC_APP_NAME=MySQL-Perf-Demo

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env.example ######
MYSQL_ROOT_PASSWORD=demo123
MYSQL_DATABASE=employees
MYSQL_USER=myuser
MYSQL_PASSWORD=demopass

# New Relic MySQL Monitor User
MYSQL_MONITOR_USER=newrelic
MYSQL_MONITOR_PASSWORD=nrpass123

# API Configuration
API_PORT=3000

# Data Generation
BATCH_SIZE=5000
TOTAL_EMPLOYEES=100000

# Performance Testing
ENABLE_SLOW_QUERIES=true
SLOW_QUERY_TIME=0.5

# Load Generator Configuration
K6_VUS=100
K6_DURATION=30m

# New Relic Configuration
NEW_RELIC_LICENSE_KEY=your_license_key
NEW_RELIC_APP_NAME=MySQL-Demo

# Environment
NODE_ENV=development

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env.local.template ######
# Database Configuration
MYSQL_HOST=host.docker.internal
MYSQL_PORT=3306
MYSQL_USER=local_user
MYSQL_PASSWORD=local_pass
MYSQL_DATABASE=employees
MYSQL_ROOT_PASSWORD=local_root_pw

# Monitor User Configuration
MYSQL_MONITOR_USER=monitor
MYSQL_MONITOR_PASSWORD=monitor_pass

# Performance Settings
BATCH_SIZE=1000
TOTAL_EMPLOYEES=5000

# API Configuration
API_PORT=3000
NODE_ENV=development

# Load Generator Configuration
K6_VUS=10
K6_DURATION=5m

# New Relic Configuration
NEW_RELIC_LICENSE_KEY=your_local_license_key
NEW_RELIC_APP_NAME=MySQL-Perf-Demo-Local

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.gitignore ######
# Environment files
*.env
!.env.example
!.env.docker
!.env.local.template
.env.local
.env.*.local
docker-compose.override.yml

# Existing entries remain unchanged
*.log
logs/
mysql_logs/
node_modules/
__pycache__/
*.pyc
.vscode/
.idea/
*.swp
*.swo
*~
mysql_data/
data/
.docker/
*.tmp
*.temp
.DS_Store


###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\docker-compose.yml ######
services:
  mysql:
    build:
      context: ./db-setup
      dockerfile: Dockerfile
    container_name: mysql-db
    profiles: ["docker"]
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      MYSQL_USER: ${MYSQL_USER}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
      MYSQL_MONITOR_USER: ${MYSQL_MONITOR_USER}
      MYSQL_MONITOR_PASSWORD: ${MYSQL_MONITOR_PASSWORD}
    ports:
      - "${MYSQL_PORT:-3306}:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - mysql_logs:/var/log/mysql
      - ./db-setup/config:/etc/mysql/conf.d
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 2G
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "$MYSQL_USER", "-p$MYSQL_PASSWORD"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    command: ['mysqld', '--character-set-server=utf8mb4', '--collation-server=utf8mb4_unicode_ci']

  api:
    build:
      context: ./services/api
      dockerfile: Dockerfile
    container_name: employees-api
    profiles: ["docker", "local"]
    environment:
      MYSQL_HOST: ${MYSQL_HOST}
      MYSQL_PORT: ${MYSQL_PORT:-3306}
      MYSQL_USER: ${MYSQL_USER}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      API_PORT: ${API_PORT}
      NEW_RELIC_LICENSE_KEY: ${NEW_RELIC_LICENSE_KEY}
      NEW_RELIC_APP_NAME: ${NEW_RELIC_APP_NAME}
      NODE_ENV: ${NODE_ENV:-production}
    ports:
      - "${API_PORT}:3000"
    networks:
      - backend
    depends_on:
      mysql:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
    healthcheck:
      test: curl -f http://localhost:3000/health || exit 1
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 15s

  load-generator:
    build:
      context: ./services/load-generator
      dockerfile: Dockerfile
    profiles: ["docker", "local"]
    environment:
      API_URL: http://api:3000
      K6_VUS: ${K6_VUS:-10}
      K6_DURATION: ${K6_DURATION:-5m}
    networks:
      - backend
    depends_on:
      api:
        condition: service_healthy

networks:
  backend:
    driver: bridge

volumes:
  mysql_data:
  mysql_logs:
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\README.md ######
## Deployment Options

### Docker-based MySQL (Default)
```bash
./scripts/run.sh docker
```

### Local MySQL Setup
1. Install MySQL locally
2. Run setup script:
```bash
./scripts/setup-local.sh
# Edit .env.local with your credentials
```

3. Start services:
```bash
./scripts/run.sh local
```

### Configuration Files
- `.env.docker`: Docker MySQL configuration
- `.env.local`: Local MySQL settings (created from template)
- `.env.local.template`: Template for local setup

### Switching Environments
- To use Docker MySQL: `./scripts/run.sh docker`
- To use local MySQL: `./scripts/run.sh local`

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\config\mysql.cnf ######
# MySQL Configuration
[mysqld]
bind-address = 0.0.0.0
port = 3306
default-storage-engine = InnoDB
innodb_buffer_pool_size = 1G

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\config\performance-schema.cnf ######
# Performance Schema Configuration
[mysqld]
performance_schema = ON
performance_schema_instrument = 'statement/sql/*=ON'
performance_schema_consumer_events_statements_history = ON
performance_schema_consumer_events_statements_history_long = ON

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\combined_content.txt ######
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\combined_content.txt ######

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\Dockerfile ######
FROM mysql:8.0

# Install required utilities
RUN microdnf update && \
    microdnf install -y curl python3 python3-pip && \
    microdnf clean all

# Create required directories and set permissions
RUN mkdir -p /docker-entrypoint-initdb.d /flyway/sql /scripts /var/log/mysql /var/lib/mysql-files && \
    chown mysql:mysql /var/log/mysql && \
    chown mysql:mysql /var/lib/mysql-files && \
    chmod 750 /var/log/mysql && \
    chmod 750 /var/lib/mysql-files

# Copy Flyway migrations
COPY flyway/sql/ /flyway/sql/

# Copy Python scripts and requirements
COPY scripts/ /scripts/
COPY requirements.txt /scripts/

# Copy configuration files and set permissions
COPY config/mysql.cnf /etc/mysql/conf.d/
RUN chmod 644 /etc/mysql/conf.d/*.cnf && \
    chown -R mysql:mysql /etc/mysql/conf.d/

# Install Python dependencies
RUN pip3 install --no-cache-dir -r /scripts/requirements.txt

# Copy custom initialization script
COPY scripts/init-db.sh /docker-entrypoint-initdb.d/

# Set permissions
RUN chmod +x /docker-entrypoint-initdb.d/init-db.sh && \
    chmod -R 755 /scripts

EXPOSE 3306

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD mysqladmin ping -h"localhost" -u"$MYSQL_USER" -p"$MYSQL_PASSWORD" --silent
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\my.cnf ######
[mysqld]
default_authentication_plugin=mysql_native_password
explicit_defaults_for_timestamp=1

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\requirements.txt ######
mysql-connector-python>=8.0.0
Faker>=8.0.0
python-dateutil>=2.8.0
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\config\mysql.cnf ######
[mysqld]
# InnoDB Settings for Performance Impact
innodb_buffer_pool_size = 256M
innodb_buffer_pool_instances = 1
innodb_buffer_pool_chunk_size = 128M
innodb_io_capacity = 100
innodb_io_capacity_max = 200
innodb_flush_log_at_trx_commit = 1
innodb_flush_method = O_DIRECT
innodb_thread_concurrency = 4
innodb_lock_wait_timeout = 20
innodb_deadlock_detect = ON
innodb_print_all_deadlocks = ON

# Buffer Settings for Memory Events
sort_buffer_size = 64K
read_buffer_size = 64K
join_buffer_size = 64K
tmp_table_size = 1M
max_heap_table_size = 1M

# Performance Schema Settings
performance_schema = ON
performance_schema_max_digest_length = 4096
performance_schema_max_sql_text_length = 4096

# Monitoring instruments
performance_schema_instrument = 'wait/io/file/innodb/innodb_data_file=ON'
performance_schema_instrument = 'wait/io/file/innodb/innodb_log_file=ON'
performance_schema_instrument = 'wait/lock/table/sql/handler=ON'
performance_schema_instrument = 'wait/lock/metadata/sql/mdl=ON'
performance_schema_instrument = 'stage/innodb/buffer pool load=ON'
performance_schema_instrument = 'memory/innodb/buf_buf_pool=ON'

# Slow Query Log
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 1
log_queries_not_using_indexes = 1

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\flyway\sql\V1__init_schema.sql ######
-- Add this after the employees table creation but before other tables
ALTER TABLE employees 
ADD UNIQUE KEY uk_emp_no (emp_no);

-- Modify the tablespace creation from:
CREATE TABLESPACE perf_space

-- To:
CREATE TABLESPACE IF NOT EXISTS perf_space

-- Rest of the original SQL script content

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\flyway\sql\V2__create_users.sql ######
-- Create users with basic permissions
CREATE USER IF NOT EXISTS '${MYSQL_USER}'@'%' IDENTIFIED BY '${MYSQL_PASSWORD}';
CREATE USER IF NOT EXISTS '${MYSQL_MONITOR_USER}'@'%' IDENTIFIED BY '${MYSQL_MONITOR_PASSWORD}';

-- Grant permissions to application user
GRANT ALL PRIVILEGES ON ${MYSQL_DATABASE}.* TO '${MYSQL_USER}'@'%';

-- Grant monitoring permissions
GRANT SELECT, PROCESS, REPLICATION CLIENT ON *.* TO '${MYSQL_MONITOR_USER}'@'%';
GRANT SELECT ON performance_schema.* TO '${MYSQL_MONITOR_USER}'@'%';
GRANT SELECT ON sys.* TO '${MYSQL_MONITOR_USER}'@'%';
GRANT SELECT ON information_schema.* TO '${MYSQL_MONITOR_USER}'@'%';

-- Enable performance monitoring
UPDATE performance_schema.setup_instruments 
SET ENABLED = 'YES', TIMED = 'YES'
WHERE NAME LIKE '%statement/%' 
   OR NAME LIKE '%stage/%'
   OR NAME LIKE '%wait/%'
   OR NAME LIKE '%memory/%';

UPDATE performance_schema.setup_consumers
SET ENABLED = 'YES'
WHERE NAME LIKE '%events%';

FLUSH PRIVILEGES;

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\flyway\sql\V3__setup_monitoring.sql ######
-- Enable ALL performance monitoring instruments
UPDATE performance_schema.setup_instruments 
SET ENABLED = 'YES', TIMED = 'YES';

-- Enable ALL consumers
UPDATE performance_schema.setup_consumers
SET ENABLED = 'YES';

-- Specific instrument categories (for clarity of what's being monitored)
UPDATE performance_schema.setup_instruments 
SET ENABLED = 'YES', TIMED = 'YES'
WHERE NAME LIKE 'wait/%'
   OR NAME LIKE 'stage/%'
   OR NAME LIKE 'statement/%'
   OR NAME LIKE 'transaction/%'
   OR NAME LIKE 'memory/%'
   OR NAME LIKE 'idle/%'
   OR NAME LIKE 'error/%'
   OR NAME LIKE 'lock/%'
   OR NAME LIKE 'metadata/%'
   OR NAME LIKE 'table/%'
   OR NAME LIKE 'socket/%'
   OR NAME LIKE 'prepared_statements/%'
   OR NAME LIKE 'program/%'
   OR NAME LIKE 'host_cache/%'
   OR NAME LIKE 'threads/%';

-- Enable thread monitoring
UPDATE performance_schema.setup_threads
SET ENABLED = 'YES', HISTORY = 'YES';

-- Enable index stats
UPDATE performance_schema.setup_objects 
SET ENABLED = 'YES', TIMED = 'YES'
WHERE OBJECT_TYPE = 'TABLE';

-- Set global variables for additional monitoring
SET GLOBAL log_output = 'TABLE,FILE';
SET GLOBAL general_log = 'ON';
SET GLOBAL slow_query_log = 'ON';

-- Create poor histograms (as before)
ANALYZE TABLE employees UPDATE HISTOGRAM ON 
    birth_date, hire_date, gender 
WITH 2 BUCKETS;

ANALYZE TABLE salaries UPDATE HISTOGRAM ON
    salary, from_date, to_date
WITH 2 BUCKETS;

-- Flush privileges to ensure changes take effect
FLUSH PRIVILEGES;
FLUSH STATUS;

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\departments_data.py ######
#!/usr/bin/env python3
import mysql.connector
import os
import logging
import json
from faker import Faker
from mysql.connector import Error

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DEPARTMENTS = [
    ('d001', 'Marketing'),
    ('d002', 'Finance'),
    ('d003', 'Human Resources'),
    ('d004', 'Research and Development'),
    ('d005', 'Quality Assurance'),
    ('d006', 'Sales'),
    ('d007', 'IT'),
    ('d008', 'Operations'),
    ('d009', 'Customer Support'),
    ('d010', 'Product Management')
]

def get_env_or_fail(var_name: str) -> str:
    value = os.getenv(var_name)
    if value is None:
        raise ValueError(f"Required environment variable {var_name} is not set")
    return value

def get_db_config():
    return {
        'host': os.getenv('MYSQL_HOST', 'localhost'),
        'user': get_env_or_fail('MYSQL_USER'),
        'password': get_env_or_fail('MYSQL_PASSWORD'),
        'database': get_env_or_fail('MYSQL_DATABASE'),
        'raise_on_warnings': True
    }

def main():
    fake = Faker()
    try:
        conn = mysql.connector.connect(**get_db_config())
        cursor = conn.cursor()
        
        # Insert departments with rich data
        enriched_departments = [
            (
                dept_no,
                dept_name,
                fake.text(max_nb_chars=500),  # department_description
                json.dumps({
                    'created_by': 'system',
                    'created_at': fake.date_time().isoformat(),
                    'metadata': {
                        'location': fake.city(),
                        'cost_center': fake.random_number(digits=6),
                        'reporting_line': fake.name()
                    }
                })  # audit_log
            )
            for dept_no, dept_name in DEPARTMENTS
        ]
        
        cursor.executemany(
            """INSERT INTO departments 
               (dept_no, dept_name, department_description, audit_log)
               VALUES (%s, %s, %s, %s)
               ON DUPLICATE KEY UPDATE 
               dept_name = VALUES(dept_name),
               department_description = VALUES(department_description),
               audit_log = VALUES(audit_log)""",
            enriched_departments
        )
        
        # Initialize manager_budget randomly
        cursor.execute("""
            UPDATE departments 
            SET manager_budget = FLOOR(1000000 + RAND() * 1000000)
            WHERE manager_budget IS NULL
        """)
        
        conn.commit()
        logger.info(f"Inserted {len(DEPARTMENTS)} departments successfully")
        
    except Error as e:
        logger.error(f"Error: {e}")
        if 'conn' in locals():
            conn.rollback()
        raise
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()

if __name__ == "__main__":
    main()
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\healthcheck.sh ######
#!/bin/bash
set -eo pipefail

if ! mysqladmin ping -h"localhost" -u"${MYSQL_USER}" -p"${MYSQL_PASSWORD}" --silent; then
    exit 1
fi

mysql -u"${MYSQL_USER}" -p"${MYSQL_PASSWORD}" -e "SELECT 1;" >/dev/null 2>&1
exit $?
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\init-db.sh ######
#!/bin/bash
set -e

# Wait for MySQL to be ready
while ! mysqladmin ping -h"localhost" -u"root" -p"${MYSQL_ROOT_PASSWORD}" --silent; do
    echo "Waiting for MySQL to be ready..."
    sleep 2
done

# Run Flyway migrations in order
echo "Running Flyway migrations..."
for sql_file in /flyway/sql/V*__*.sql; do
    echo "Executing $sql_file..."
    mysql -u root -p"${MYSQL_ROOT_PASSWORD}" < "$sql_file"
done

# Load initial data
echo "Loading department data..."
cd /scripts && python3 departments_data.py

echo "Loading employee data..."
cd /scripts && python3 load_data.py

echo "Database initialization completed!"
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\load_data.py ######
#!/usr/bin/env python3
import mysql.connector
import random
import json
import decimal
from datetime import date, timedelta
from faker import Faker
import os
import logging
import time
from mysql.connector import Error, IntegrityError
from mysql.connector.errors import OperationalError, InterfaceError
from typing import List, Tuple, Dict, Any, Set
import sys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def get_db_config() -> Dict[str, Any]:
    """Database configuration with retries and timeouts."""
    return {
        'host': os.getenv('MYSQL_HOST', 'localhost'),
        'user': os.getenv('MYSQL_USER'),
        'password': os.getenv('MYSQL_PASSWORD'),
        'database': os.getenv('MYSQL_DATABASE'),
        'raise_on_warnings': True,
        'connect_timeout': 30,
        'connection_timeout': 30,
        'pool_size': 5,
        'pool_name': 'data_loader',
        'pool_reset_session': True,
        'allow_local_infile': True
    }

def create_database_connection(max_retries: int = 5, retry_delay: int = 5) -> mysql.connector.MySQLConnection:
    """Create database connection with retry logic."""
    for attempt in range(max_retries):
        try:
            conn = mysql.connector.connect(**get_db_config())
            cursor = conn.cursor()
            
            # Configure session for performance testing
            cursor.execute("SET SESSION innodb_flush_log_at_trx_commit = 1")
            cursor.execute("SET SESSION foreign_key_checks = 0")
            cursor.execute("SET SESSION unique_checks = 0")
            cursor.execute("SET SESSION sql_log_bin = 1")
            
            cursor.close()
            return conn
            
        except mysql.connector.Error as err:
            if attempt == max_retries - 1:
                logger.error(f"Failed to connect to database after {max_retries} attempts")
                raise Exception("Max retries exceeded while trying to connect to the database.")
            logger.warning(f"Connection attempt {attempt + 1} failed: {err}")
            time.sleep(retry_delay)

def generate_employee_batch(
    start_emp_no: int,
    batch_size: int,
    fake: Faker,
    used_emp_nos: Set[int]
) -> Tuple[List[Tuple], List[Tuple], List[Tuple]]:
    """Generate a batch of employee data with related salary and department records."""
    employees = []
    salaries = []
    dept_assignments = []
    departments = ['d001', 'd002', 'd003', 'd004', 'd005', 
                  'd006', 'd007', 'd008', 'd009', 'd010']
    
    for i in range(batch_size):
        # Generate unique non-sequential employee number
        while True:
            emp_no = start_emp_no + random.randint(1, 1000)
            if emp_no not in used_emp_nos:
                used_emp_nos.add(emp_no)
                break

        # Generate dates with realistic constraints
        birth_date = fake.date_between(start_date='-65y', end_date='-25y')
        hire_date = fake.date_between(
            start_date=max(birth_date + timedelta(days=6570), date(2000, 1, 1)),
            end_date='today'
        )

        # Generate rich employee details
        employee_details = {
            'address': fake.address(),
            'phone': fake.phone_number(),
            'email': fake.email(),
            'emergency_contact': {
                'name': fake.name(),
                'relationship': random.choice(['Spouse', 'Parent', 'Sibling', 'Friend']),
                'phone': fake.phone_number()
            },
            'skills': [fake.job_skill() for _ in range(random.randint(3, 8))],
            'education': [
                {
                    'degree': fake.random_element(['BS', 'MS', 'PhD', 'BA', 'MBA']),
                    'field': fake.random_element(['Computer Science', 'Engineering', 'Business', 'Mathematics']),
                    'year': random.randint(1980, 2020),
                    'institution': fake.university()
                } for _ in range(random.randint(1, 3))
            ],
            'languages': [
                {
                    'language': lang,
                    'proficiency': random.choice(['Basic', 'Intermediate', 'Advanced', 'Native'])
                } for lang in random.sample(['English', 'Spanish', 'French', 'German', 'Chinese', 'Japanese'], 
                                         random.randint(1, 3))
            ]
        }

        # Generate detailed performance history
        performance_history = {
            'reviews': [
                {
                    'date': fake.date_time_between(start_date=hire_date).isoformat(),
                    'rating': random.randint(1, 5),
                    'feedback': fake.text(max_nb_chars=500),
                    'reviewer': fake.name(),
                    'categories': {
                        'technical_skills': random.randint(1, 5),
                        'communication': random.randint(1, 5),
                        'teamwork': random.randint(1, 5),
                        'initiative': random.randint(1, 5)
                    }
                } for _ in range(random.randint(3, 8))
            ],
            'promotions': [
                {
                    'date': fake.date_between(start_date=hire_date).isoformat(),
                    'new_title': fake.job_title(),
                    'previous_title': fake.job_title(),
                    'reason': fake.text(max_nb_chars=200),
                    'salary_impact': f"{random.randint(5, 25)}%"
                } for _ in range(random.randint(0, 3))
            ],
            'training': [
                {
                    'course': fake.random_element(['Leadership', 'Technical Skills', 'Soft Skills', 'Project Management']),
                    'completion_date': fake.date_between(start_date=hire_date).isoformat(),
                    'score': random.randint(70, 100)
                } for _ in range(random.randint(2, 6))
            ]
        }

        employees.append((
            emp_no,
            birth_date,
            fake.first_name(),
            fake.last_name(),
            random.choice(['M', 'F']),
            hire_date,
            json.dumps(employee_details),
            json.dumps(performance_history)
        ))

        # Generate salary history with more varied patterns
        current_date = hire_date
        base_salary = decimal.Decimal(random.randint(30000, 70000))
        
        for year in range(random.randint(3, 8)):
            # Allow for salary decreases and varied increases
            salary_change = decimal.Decimal(str(random.uniform(-0.1, 0.2)))
            salary = base_salary * (decimal.Decimal('1.0') + salary_change)
            
            from_date = current_date
            to_date = current_date + timedelta(days=random.randint(180, 720))
            
            salary_details = {
                'base_salary': float(base_salary),
                'bonus_percent': random.uniform(0.05, 0.15),
                'allowances': {
                    'housing': random.randint(5000, 15000),
                    'transportation': random.randint(1000, 3000),
                    'medical': random.randint(2000, 8000)
                },
                'deductions': {
                    'tax': random.uniform(0.15, 0.25),
                    'insurance': random.uniform(0.05, 0.1),
                    'pension': random.uniform(0.03, 0.08)
                },
                'performance_bonus': random.randint(0, 10000) if random.random() < 0.3 else 0
            }

            audit_trail = {
                'modified_by': fake.name(),
                'modified_at': fake.date_time().isoformat(),
                'reason': fake.text(max_nb_chars=100),
                'previous_salary': float(base_salary),
                'change_percent': float(salary_change * 100),
                'approval_chain': [fake.name() for _ in range(random.randint(1, 3))],
                'market_adjustment': random.choice([True, False]),
                'review_period': f"FY{random.randint(2000, 2024)}"
            }

            salaries.append((
                emp_no,
                salary.quantize(decimal.Decimal('.01')),
                from_date,
                to_date,
                json.dumps(salary_details),
                json.dumps(audit_trail)
            ))
            
            current_date = to_date - timedelta(days=random.randint(0, 30))  # Allow some overlap
            base_salary = salary

        # Generate overlapping department assignments
        num_assignments = random.randint(2, 4)
        available_departments = departments.copy()
        
        for _ in range(num_assignments):
            if not available_departments:
                break
                
            dept_no = random.choice(available_departments)
            available_departments.remove(dept_no)
            
            # Create intentionally overlapping dates
            from_date = hire_date + timedelta(days=random.randint(-30, 30))
            to_date = from_date + timedelta(days=random.randint(180, 720))
            
            dept_assignments.append((
                emp_no,
                dept_no,
                from_date,
                to_date
            ))

    return employees, salaries, dept_assignments

def main():
    # Cap batch size for manageability
    batch_size = min(int(os.getenv('BATCH_SIZE', 1000)), 5000)
    total_employees = int(os.getenv('TOTAL_EMPLOYEES', 10000))
    
    fake = Faker()
    used_emp_nos = set()
    
    try:
        conn = create_database_connection()
        cursor = conn.cursor()
        
        logger.info(f"Starting data load: target {total_employees} employees")
        start_time = time.time()
        
        for batch_start in range(0, total_employees, batch_size):
            batch_time = time.time()
            current_batch_size = min(batch_size, total_employees - batch_start)
            
            try:
                employees, salaries, dept_assignments = generate_employee_batch(
                    batch_start + 1000000,
                    current_batch_size,
                    fake,
                    used_emp_nos
                )
                
                # Insert with error handling
                try:
                    cursor.executemany(
                        """INSERT INTO employees 
                           (emp_no, birth_date, first_name, last_name, gender, hire_date, 
                            employee_details, performance_history)
                           VALUES (%s, %s, %s, %s, %s, %s, %s, %s)""",
                        employees
                    )
                    
                    cursor.executemany(
                        """INSERT INTO salaries 
                           (emp_no, salary, from_date, to_date, salary_details, audit_trail)
                           VALUES (%s, %s, %s, %s, %s, %s)""",
                        salaries
                    )
                    
                    cursor.executemany(
                        """INSERT INTO dept_emp 
                           (emp_no, dept_no, from_date, to_date)
                           VALUES (%s, %s, %s, %s)""",
                        dept_assignments
                    )
                    
                    conn.commit()
                    
                except IntegrityError as e:
                    logger.warning(f"Integrity error in batch: {e}")
                    conn.rollback()
                    continue
                
                batch_elapsed = time.time() - batch_time
                total_elapsed = time.time() - start_time
                
                logger.info(
                    f"Batch {batch_start//batch_size + 1} completed: "
                    f"{len(employees)} employees, "
                    f"{len(salaries)} salary records, "
                    f"{len(dept_assignments)} department assignments. "
                    f"Batch time: {batch_elapsed:.2f}s, "
                    f"Total time: {total_elapsed:.2f}s"
                )
                
                # Occasionally analyze tables
                if random.random() < 0.1:
                    logger.info("Running table analysis...")
                    cursor.execute("ANALYZE TABLE employees, salaries, dept_emp")
                
            except Error as e:
                logger.error(f"Error in batch starting at {batch_start}: {e}")
                conn.rollback()
                continue
            
    except Error as e:
        logger.error(f"Database error: {e}")
        raise
    finally:
        if 'cursor' in locals():
            cursor.execute("SET SESSION foreign_key_checks = 1")
            cursor.execute("SET SESSION unique_checks = 1")
            cursor.close()
        if 'conn' in locals():
            conn.close()
            
    total_time = time.time() - start_time
    logger.info(f"Data load completed in {total_time:.2f} seconds")

if __name__ == "__main__":
    main()
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\run_load_data.sh ######
#!/bin/bash
set -e

echo "Waiting for MySQL to be ready..."
while ! mysqladmin ping -h"localhost" --silent; do
    sleep 2
done

echo "Starting data load process..."
python3 /docker-entrypoint-initdb.d/load_data.py


###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\Dockerfile ######
FROM mysql:8.0

# Install required utilities
RUN microdnf update && \
    microdnf install -y curl python3 python3-pip && \
    microdnf clean all

# Create required directories and set permissions
RUN mkdir -p /docker-entrypoint-initdb.d /flyway/sql /scripts /var/log/mysql /var/lib/mysql-files && \
    chown mysql:mysql /var/log/mysql && \
    chown mysql:mysql /var/lib/mysql-files && \
    chmod 750 /var/log/mysql && \
    chmod 750 /var/lib/mysql-files

# Copy Flyway migrations
COPY flyway/sql/ /flyway/sql/

# Copy Python scripts and requirements
COPY scripts/ /scripts/
COPY requirements.txt /scripts/

# Copy configuration files and set permissions
COPY config/mysql.cnf /etc/mysql/conf.d/
RUN chmod 644 /etc/mysql/conf.d/*.cnf && \
    chown -R mysql:mysql /etc/mysql/conf.d/

# Install Python dependencies
RUN pip3 install --no-cache-dir -r /scripts/requirements.txt

# Copy custom initialization script
COPY scripts/init-db.sh /docker-entrypoint-initdb.d/

# Set permissions
RUN chmod +x /docker-entrypoint-initdb.d/init-db.sh && \
    chmod -R 755 /scripts

EXPOSE 3306

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD mysqladmin ping -h"localhost" -u"$MYSQL_USER" -p"$MYSQL_PASSWORD" --silent
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\my.cnf ######
[mysqld]
default_authentication_plugin=mysql_native_password
explicit_defaults_for_timestamp=1

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\requirements.txt ######
mysql-connector-python>=8.0.0
Faker>=8.0.0
python-dateutil>=2.8.0
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\config\mysql.cnf ######
[mysqld]
# InnoDB Settings for Performance Impact
innodb_buffer_pool_size = 256M
innodb_buffer_pool_instances = 1
innodb_buffer_pool_chunk_size = 128M
innodb_io_capacity = 100
innodb_io_capacity_max = 200
innodb_flush_log_at_trx_commit = 1
innodb_flush_method = O_DIRECT
innodb_thread_concurrency = 4
innodb_lock_wait_timeout = 20
innodb_deadlock_detect = ON
innodb_print_all_deadlocks = ON

# Buffer Settings for Memory Events
sort_buffer_size = 64K
read_buffer_size = 64K
join_buffer_size = 64K
tmp_table_size = 1M
max_heap_table_size = 1M

# Performance Schema Settings
performance_schema = ON
performance_schema_max_digest_length = 4096
performance_schema_max_sql_text_length = 4096

# Monitoring instruments
performance_schema_instrument = 'wait/io/file/innodb/innodb_data_file=ON'
performance_schema_instrument = 'wait/io/file/innodb/innodb_log_file=ON'
performance_schema_instrument = 'wait/lock/table/sql/handler=ON'
performance_schema_instrument = 'wait/lock/metadata/sql/mdl=ON'
performance_schema_instrument = 'stage/innodb/buffer pool load=ON'
performance_schema_instrument = 'memory/innodb/buf_buf_pool=ON'

# Slow Query Log
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 1
log_queries_not_using_indexes = 1

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\flyway\sql\V1__init_schema.sql ######
-- Add this after the employees table creation but before other tables
ALTER TABLE employees 
ADD UNIQUE KEY uk_emp_no (emp_no);

-- Modify the tablespace creation from:
CREATE TABLESPACE perf_space

-- To:
CREATE TABLESPACE IF NOT EXISTS perf_space

-- Rest of the original SQL script content

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\flyway\sql\V2__create_users.sql ######
-- Create users with basic permissions
CREATE USER IF NOT EXISTS '${MYSQL_USER}'@'%' IDENTIFIED BY '${MYSQL_PASSWORD}';
CREATE USER IF NOT EXISTS '${MYSQL_MONITOR_USER}'@'%' IDENTIFIED BY '${MYSQL_MONITOR_PASSWORD}';

-- Grant permissions to application user
GRANT ALL PRIVILEGES ON ${MYSQL_DATABASE}.* TO '${MYSQL_USER}'@'%';

-- Grant monitoring permissions
GRANT SELECT, PROCESS, REPLICATION CLIENT ON *.* TO '${MYSQL_MONITOR_USER}'@'%';
GRANT SELECT ON performance_schema.* TO '${MYSQL_MONITOR_USER}'@'%';
GRANT SELECT ON sys.* TO '${MYSQL_MONITOR_USER}'@'%';
GRANT SELECT ON information_schema.* TO '${MYSQL_MONITOR_USER}'@'%';

-- Enable performance monitoring
UPDATE performance_schema.setup_instruments 
SET ENABLED = 'YES', TIMED = 'YES'
WHERE NAME LIKE '%statement/%' 
   OR NAME LIKE '%stage/%'
   OR NAME LIKE '%wait/%'
   OR NAME LIKE '%memory/%';

UPDATE performance_schema.setup_consumers
SET ENABLED = 'YES'
WHERE NAME LIKE '%events%';

FLUSH PRIVILEGES;

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\flyway\sql\V3__setup_monitoring.sql ######
-- Enable ALL performance monitoring instruments
UPDATE performance_schema.setup_instruments 
SET ENABLED = 'YES', TIMED = 'YES';

-- Enable ALL consumers
UPDATE performance_schema.setup_consumers
SET ENABLED = 'YES';

-- Specific instrument categories (for clarity of what's being monitored)
UPDATE performance_schema.setup_instruments 
SET ENABLED = 'YES', TIMED = 'YES'
WHERE NAME LIKE 'wait/%'
   OR NAME LIKE 'stage/%'
   OR NAME LIKE 'statement/%'
   OR NAME LIKE 'transaction/%'
   OR NAME LIKE 'memory/%'
   OR NAME LIKE 'idle/%'
   OR NAME LIKE 'error/%'
   OR NAME LIKE 'lock/%'
   OR NAME LIKE 'metadata/%'
   OR NAME LIKE 'table/%'
   OR NAME LIKE 'socket/%'
   OR NAME LIKE 'prepared_statements/%'
   OR NAME LIKE 'program/%'
   OR NAME LIKE 'host_cache/%'
   OR NAME LIKE 'threads/%';

-- Enable thread monitoring
UPDATE performance_schema.setup_threads
SET ENABLED = 'YES', HISTORY = 'YES';

-- Enable index stats
UPDATE performance_schema.setup_objects 
SET ENABLED = 'YES', TIMED = 'YES'
WHERE OBJECT_TYPE = 'TABLE';

-- Set global variables for additional monitoring
SET GLOBAL log_output = 'TABLE,FILE';
SET GLOBAL general_log = 'ON';
SET GLOBAL slow_query_log = 'ON';

-- Create poor histograms (as before)
ANALYZE TABLE employees UPDATE HISTOGRAM ON 
    birth_date, hire_date, gender 
WITH 2 BUCKETS;

ANALYZE TABLE salaries UPDATE HISTOGRAM ON
    salary, from_date, to_date
WITH 2 BUCKETS;

-- Flush privileges to ensure changes take effect
FLUSH PRIVILEGES;
FLUSH STATUS;

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\departments_data.py ######
#!/usr/bin/env python3
import mysql.connector
import os
import logging
import json
from faker import Faker
from mysql.connector import Error

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DEPARTMENTS = [
    ('d001', 'Marketing'),
    ('d002', 'Finance'),
    ('d003', 'Human Resources'),
    ('d004', 'Research and Development'),
    ('d005', 'Quality Assurance'),
    ('d006', 'Sales'),
    ('d007', 'IT'),
    ('d008', 'Operations'),
    ('d009', 'Customer Support'),
    ('d010', 'Product Management')
]

def get_env_or_fail(var_name: str) -> str:
    value = os.getenv(var_name)
    if value is None:
        raise ValueError(f"Required environment variable {var_name} is not set")
    return value

def get_db_config():
    return {
        'host': os.getenv('MYSQL_HOST', 'localhost'),
        'user': get_env_or_fail('MYSQL_USER'),
        'password': get_env_or_fail('MYSQL_PASSWORD'),
        'database': get_env_or_fail('MYSQL_DATABASE'),
        'raise_on_warnings': True
    }

def main():
    fake = Faker()
    try:
        conn = mysql.connector.connect(**get_db_config())
        cursor = conn.cursor()
        
        # Insert departments with rich data
        enriched_departments = [
            (
                dept_no,
                dept_name,
                fake.text(max_nb_chars=500),  # department_description
                json.dumps({
                    'created_by': 'system',
                    'created_at': fake.date_time().isoformat(),
                    'metadata': {
                        'location': fake.city(),
                        'cost_center': fake.random_number(digits=6),
                        'reporting_line': fake.name()
                    }
                })  # audit_log
            )
            for dept_no, dept_name in DEPARTMENTS
        ]
        
        cursor.executemany(
            """INSERT INTO departments 
               (dept_no, dept_name, department_description, audit_log)
               VALUES (%s, %s, %s, %s)
               ON DUPLICATE KEY UPDATE 
               dept_name = VALUES(dept_name),
               department_description = VALUES(department_description),
               audit_log = VALUES(audit_log)""",
            enriched_departments
        )
        
        # Initialize manager_budget randomly
        cursor.execute("""
            UPDATE departments 
            SET manager_budget = FLOOR(1000000 + RAND() * 1000000)
            WHERE manager_budget IS NULL
        """)
        
        conn.commit()
        logger.info(f"Inserted {len(DEPARTMENTS)} departments successfully")
        
    except Error as e:
        logger.error(f"Error: {e}")
        if 'conn' in locals():
            conn.rollback()
        raise
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()

if __name__ == "__main__":
    main()
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\healthcheck.sh ######
#!/bin/bash
set -eo pipefail

if ! mysqladmin ping -h"localhost" -u"${MYSQL_USER}" -p"${MYSQL_PASSWORD}" --silent; then
    exit 1
fi

mysql -u"${MYSQL_USER}" -p"${MYSQL_PASSWORD}" -e "SELECT 1;" >/dev/null 2>&1
exit $?
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\init-db.sh ######
#!/bin/bash
set -e

# Wait for MySQL to be ready
while ! mysqladmin ping -h"localhost" -u"root" -p"${MYSQL_ROOT_PASSWORD}" --silent; do
    echo "Waiting for MySQL to be ready..."
    sleep 2
done

# Run Flyway migrations in order
echo "Running Flyway migrations..."
for sql_file in /flyway/sql/V*__*.sql; do
    echo "Executing $sql_file..."
    mysql -u root -p"${MYSQL_ROOT_PASSWORD}" < "$sql_file"
done

# Load initial data
echo "Loading department data..."
cd /scripts && python3 departments_data.py

echo "Loading employee data..."
cd /scripts && python3 load_data.py

echo "Database initialization completed!"
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\load_data.py ######
#!/usr/bin/env python3
import mysql.connector
import random
import json
import decimal
from datetime import date, timedelta
from faker import Faker
import os
import logging
import time
from mysql.connector import Error, IntegrityError
from mysql.connector.errors import OperationalError, InterfaceError
from typing import List, Tuple, Dict, Any, Set
import sys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def get_db_config() -> Dict[str, Any]:
    """Database configuration with retries and timeouts."""
    return {
        'host': os.getenv('MYSQL_HOST', 'localhost'),
        'user': os.getenv('MYSQL_USER'),
        'password': os.getenv('MYSQL_PASSWORD'),
        'database': os.getenv('MYSQL_DATABASE'),
        'raise_on_warnings': True,
        'connect_timeout': 30,
        'connection_timeout': 30,
        'pool_size': 5,
        'pool_name': 'data_loader',
        'pool_reset_session': True,
        'allow_local_infile': True
    }

def create_database_connection(max_retries: int = 5, retry_delay: int = 5) -> mysql.connector.MySQLConnection:
    """Create database connection with retry logic."""
    for attempt in range(max_retries):
        try:
            conn = mysql.connector.connect(**get_db_config())
            cursor = conn.cursor()
            
            # Configure session for performance testing
            cursor.execute("SET SESSION innodb_flush_log_at_trx_commit = 1")
            cursor.execute("SET SESSION foreign_key_checks = 0")
            cursor.execute("SET SESSION unique_checks = 0")
            cursor.execute("SET SESSION sql_log_bin = 1")
            
            cursor.close()
            return conn
            
        except mysql.connector.Error as err:
            if attempt == max_retries - 1:
                logger.error(f"Failed to connect to database after {max_retries} attempts")
                raise Exception("Max retries exceeded while trying to connect to the database.")
            logger.warning(f"Connection attempt {attempt + 1} failed: {err}")
            time.sleep(retry_delay)

def generate_employee_batch(
    start_emp_no: int,
    batch_size: int,
    fake: Faker,
    used_emp_nos: Set[int]
) -> Tuple[List[Tuple], List[Tuple], List[Tuple]]:
    """Generate a batch of employee data with related salary and department records."""
    employees = []
    salaries = []
    dept_assignments = []
    departments = ['d001', 'd002', 'd003', 'd004', 'd005', 
                  'd006', 'd007', 'd008', 'd009', 'd010']
    
    for i in range(batch_size):
        # Generate unique non-sequential employee number
        while True:
            emp_no = start_emp_no + random.randint(1, 1000)
            if emp_no not in used_emp_nos:
                used_emp_nos.add(emp_no)
                break

        # Generate dates with realistic constraints
        birth_date = fake.date_between(start_date='-65y', end_date='-25y')
        hire_date = fake.date_between(
            start_date=max(birth_date + timedelta(days=6570), date(2000, 1, 1)),
            end_date='today'
        )

        # Generate rich employee details
        employee_details = {
            'address': fake.address(),
            'phone': fake.phone_number(),
            'email': fake.email(),
            'emergency_contact': {
                'name': fake.name(),
                'relationship': random.choice(['Spouse', 'Parent', 'Sibling', 'Friend']),
                'phone': fake.phone_number()
            },
            'skills': [fake.job_skill() for _ in range(random.randint(3, 8))],
            'education': [
                {
                    'degree': fake.random_element(['BS', 'MS', 'PhD', 'BA', 'MBA']),
                    'field': fake.random_element(['Computer Science', 'Engineering', 'Business', 'Mathematics']),
                    'year': random.randint(1980, 2020),
                    'institution': fake.university()
                } for _ in range(random.randint(1, 3))
            ],
            'languages': [
                {
                    'language': lang,
                    'proficiency': random.choice(['Basic', 'Intermediate', 'Advanced', 'Native'])
                } for lang in random.sample(['English', 'Spanish', 'French', 'German', 'Chinese', 'Japanese'], 
                                         random.randint(1, 3))
            ]
        }

        # Generate detailed performance history
        performance_history = {
            'reviews': [
                {
                    'date': fake.date_time_between(start_date=hire_date).isoformat(),
                    'rating': random.randint(1, 5),
                    'feedback': fake.text(max_nb_chars=500),
                    'reviewer': fake.name(),
                    'categories': {
                        'technical_skills': random.randint(1, 5),
                        'communication': random.randint(1, 5),
                        'teamwork': random.randint(1, 5),
                        'initiative': random.randint(1, 5)
                    }
                } for _ in range(random.randint(3, 8))
            ],
            'promotions': [
                {
                    'date': fake.date_between(start_date=hire_date).isoformat(),
                    'new_title': fake.job_title(),
                    'previous_title': fake.job_title(),
                    'reason': fake.text(max_nb_chars=200),
                    'salary_impact': f"{random.randint(5, 25)}%"
                } for _ in range(random.randint(0, 3))
            ],
            'training': [
                {
                    'course': fake.random_element(['Leadership', 'Technical Skills', 'Soft Skills', 'Project Management']),
                    'completion_date': fake.date_between(start_date=hire_date).isoformat(),
                    'score': random.randint(70, 100)
                } for _ in range(random.randint(2, 6))
            ]
        }

        employees.append((
            emp_no,
            birth_date,
            fake.first_name(),
            fake.last_name(),
            random.choice(['M', 'F']),
            hire_date,
            json.dumps(employee_details),
            json.dumps(performance_history)
        ))

        # Generate salary history with more varied patterns
        current_date = hire_date
        base_salary = decimal.Decimal(random.randint(30000, 70000))
        
        for year in range(random.randint(3, 8)):
            # Allow for salary decreases and varied increases
            salary_change = decimal.Decimal(str(random.uniform(-0.1, 0.2)))
            salary = base_salary * (decimal.Decimal('1.0') + salary_change)
            
            from_date = current_date
            to_date = current_date + timedelta(days=random.randint(180, 720))
            
            salary_details = {
                'base_salary': float(base_salary),
                'bonus_percent': random.uniform(0.05, 0.15),
                'allowances': {
                    'housing': random.randint(5000, 15000),
                    'transportation': random.randint(1000, 3000),
                    'medical': random.randint(2000, 8000)
                },
                'deductions': {
                    'tax': random.uniform(0.15, 0.25),
                    'insurance': random.uniform(0.05, 0.1),
                    'pension': random.uniform(0.03, 0.08)
                },
                'performance_bonus': random.randint(0, 10000) if random.random() < 0.3 else 0
            }

            audit_trail = {
                'modified_by': fake.name(),
                'modified_at': fake.date_time().isoformat(),
                'reason': fake.text(max_nb_chars=100),
                'previous_salary': float(base_salary),
                'change_percent': float(salary_change * 100),
                'approval_chain': [fake.name() for _ in range(random.randint(1, 3))],
                'market_adjustment': random.choice([True, False]),
                'review_period': f"FY{random.randint(2000, 2024)}"
            }

            salaries.append((
                emp_no,
                salary.quantize(decimal.Decimal('.01')),
                from_date,
                to_date,
                json.dumps(salary_details),
                json.dumps(audit_trail)
            ))
            
            current_date = to_date - timedelta(days=random.randint(0, 30))  # Allow some overlap
            base_salary = salary

        # Generate overlapping department assignments
        num_assignments = random.randint(2, 4)
        available_departments = departments.copy()
        
        for _ in range(num_assignments):
            if not available_departments:
                break
                
            dept_no = random.choice(available_departments)
            available_departments.remove(dept_no)
            
            # Create intentionally overlapping dates
            from_date = hire_date + timedelta(days=random.randint(-30, 30))
            to_date = from_date + timedelta(days=random.randint(180, 720))
            
            dept_assignments.append((
                emp_no,
                dept_no,
                from_date,
                to_date
            ))

    return employees, salaries, dept_assignments

def main():
    # Cap batch size for manageability
    batch_size = min(int(os.getenv('BATCH_SIZE', 1000)), 5000)
    total_employees = int(os.getenv('TOTAL_EMPLOYEES', 10000))
    
    fake = Faker()
    used_emp_nos = set()
    
    try:
        conn = create_database_connection()
        cursor = conn.cursor()
        
        logger.info(f"Starting data load: target {total_employees} employees")
        start_time = time.time()
        
        for batch_start in range(0, total_employees, batch_size):
            batch_time = time.time()
            current_batch_size = min(batch_size, total_employees - batch_start)
            
            try:
                employees, salaries, dept_assignments = generate_employee_batch(
                    batch_start + 1000000,
                    current_batch_size,
                    fake,
                    used_emp_nos
                )
                
                # Insert with error handling
                try:
                    cursor.executemany(
                        """INSERT INTO employees 
                           (emp_no, birth_date, first_name, last_name, gender, hire_date, 
                            employee_details, performance_history)
                           VALUES (%s, %s, %s, %s, %s, %s, %s, %s)""",
                        employees
                    )
                    
                    cursor.executemany(
                        """INSERT INTO salaries 
                           (emp_no, salary, from_date, to_date, salary_details, audit_trail)
                           VALUES (%s, %s, %s, %s, %s, %s)""",
                        salaries
                    )
                    
                    cursor.executemany(
                        """INSERT INTO dept_emp 
                           (emp_no, dept_no, from_date, to_date)
                           VALUES (%s, %s, %s, %s)""",
                        dept_assignments
                    )
                    
                    conn.commit()
                    
                except IntegrityError as e:
                    logger.warning(f"Integrity error in batch: {e}")
                    conn.rollback()
                    continue
                
                batch_elapsed = time.time() - batch_time
                total_elapsed = time.time() - start_time
                
                logger.info(
                    f"Batch {batch_start//batch_size + 1} completed: "
                    f"{len(employees)} employees, "
                    f"{len(salaries)} salary records, "
                    f"{len(dept_assignments)} department assignments. "
                    f"Batch time: {batch_elapsed:.2f}s, "
                    f"Total time: {total_elapsed:.2f}s"
                )
                
                # Occasionally analyze tables
                if random.random() < 0.1:
                    logger.info("Running table analysis...")
                    cursor.execute("ANALYZE TABLE employees, salaries, dept_emp")
                
            except Error as e:
                logger.error(f"Error in batch starting at {batch_start}: {e}")
                conn.rollback()
                continue
            
    except Error as e:
        logger.error(f"Database error: {e}")
        raise
    finally:
        if 'cursor' in locals():
            cursor.execute("SET SESSION foreign_key_checks = 1")
            cursor.execute("SET SESSION unique_checks = 1")
            cursor.close()
        if 'conn' in locals():
            conn.close()
            
    total_time = time.time() - start_time
    logger.info(f"Data load completed in {total_time:.2f} seconds")

if __name__ == "__main__":
    main()
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\run_load_data.sh ######
#!/bin/bash
set -e

echo "Waiting for MySQL to be ready..."
while ! mysqladmin ping -h"localhost" --silent; do
    sleep 2
done

echo "Starting data load process..."
python3 /docker-entrypoint-initdb.d/load_data.py

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\deploy\azure\deploy.sh ######
#!/bin/bash
set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "${SCRIPT_DIR}/../common/validate.sh"

# Validate environment
validate_environment "azure"

# Load environment variables
source "${SCRIPT_DIR}/../../config/azure.env"

echo "Starting Azure VM native deployment..."

# Install MySQL if not present
if ! command -v mysql &> /dev/null; then
    echo "Installing MySQL..."
    sudo microdnf update
    sudo microdnf install -y mysql-server
fi

# Configure MySQL
echo "Configuring MySQL..."
sudo cp "${SCRIPT_DIR}/../../config/mysql/azure.cnf" /etc/mysql/conf.d/performance.cnf
sudo systemctl restart mysql

# Initialize database
echo "Initializing database..."
mysql -u root -p"${MYSQL_ROOT_PASSWORD}" < "${SCRIPT_DIR}/../../db-setup/migrations/V1__init_monitoring.sql"
mysql -u root -p"${MYSQL_ROOT_PASSWORD}" < "${SCRIPT_DIR}/../../db-setup/migrations/V2__create_base_schema.sql"
mysql -u root -p"${MYSQL_ROOT_PASSWORD}" < "${SCRIPT_DIR}/../../db-setup/migrations/V3__add_indexes.sql"
mysql -u root -p"${MYSQL_ROOT_PASSWORD}" < "${SCRIPT_DIR}/../../db-setup/migrations/V4__create_views.sql"

# Initialize data
echo "Initializing data..."
source "${SCRIPT_DIR}/../common/init-data.sh"

# Setup monitoring
echo "Setting up monitoring..."
source "${SCRIPT_DIR}/setup-monitoring.sh"

echo "Deployment complete!"

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\deploy\common\init-data.sh ######
#!/bin/bash

# Sample script to initialize data
# Add your data initialization commands here

echo "Initializing sample data..."
# Example command to insert data
# mysql -u root -p"${MYSQL_ROOT_PASSWORD}" -e "INSERT INTO employees (name) VALUES ('John Doe');"

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\deploy\common\validate.sh ######
#!/bin/bash

validate_environment() {
    local deploy_type=$1
    
    # Check for required tools
    case $deploy_type in
        "docker")
            command -v docker >/dev/null 2>&1 || { echo "Docker is required but not installed."; exit 1; }
            command -v docker-compose >/dev/null 2>&1 || { echo "Docker Compose is required but not installed."; exit 1; }
            ;; 
        "azure")
            command -v mysql >/dev/null 2>&1 || { echo "MySQL client is required but not installed."; exit 1; }
            ;;
    esac
    
    # Check for required files
    local required_files=(
        "../../config/mysql/${deploy_type}.cnf"
        "../../config/${deploy_type}.env"
        "../../db-setup/migrations/V1__init_monitoring.sql"
    )
    
    for file in "${required_files[@]}"; do
        if [[ ! -f "${SCRIPT_DIR}/${file}" ]]; then
            echo "Required file not found: ${file}"
            exit 1
        fi
    done
}

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\deploy\docker\deploy.sh ######
#!/bin/bash
set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "${SCRIPT_DIR}/../common/validate.sh"

# Validate environment
validate_environment "docker"

# Load environment variables
source "${SCRIPT_DIR}/../../config/docker.env"

echo "Starting Docker deployment..."

# Ensure Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "Docker is not running. Starting Docker..."
    sudo systemctl start docker
fi

# Pull required images
echo "Pulling required Docker images..."
docker-compose pull

# Start services
echo "Starting services..."
docker-compose up -d mysql

# Wait for MySQL to be ready
echo "Waiting for MySQL to be ready..."
until docker-compose exec -T mysql mysqladmin ping -h"localhost" -u"$MYSQL_USER" -p"$MYSQL_PASSWORD" --silent; do
    echo "MySQL is unavailable - sleeping"
    sleep 5
done

# Initialize data
echo "Initializing data..."
source "${SCRIPT_DIR}/../common/init-data.sh"

# Start remaining services
echo "Starting API and load generator..."
docker-compose up -d api load-generator

echo "Deployment complete!"

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\infrastructure\newrelic\custom-monitors.yml ######
custom_metrics:
  - query: |
      SELECT COUNT(*) as current_salaries
      FROM salaries 
      WHERE to_date = '9999-01-01'
    interval: 60

  - query: |
      SELECT COUNT(*) as active_departments
      FROM dept_emp 
      WHERE to_date = '9999-01-01'
    interval: 60

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\infrastructure\newrelic\Dockerfile ######
# Build stage for nri-mysql using a specific commit and modifying code
FROM golang:1.21-alpine AS builder

# Install git and build essentials
RUN apk add --no-cache git make

# Hardcode the integration ref
ARG MYSQL_INTEGRATION_REF=703b1f6

WORKDIR /go/src/github.com/newrelic/nri-mysql

# Clone the repository at the specified commit
RUN git clone https://github.com/spathlavath/nri-mysql.git . && \
    git checkout ${MYSQL_INTEGRATION_REF}

# Use sed to remove the conditional block and always call PopulateQueryPerformanceMetrics
RUN sed -i '/if args.EnableQueryPerformanceMonitoring {/,/}/c\query_performance_details.PopulateQueryPerformanceMetrics(args, e, i)' src/mysql.go

# Compile the modified binary
RUN make compile

# Final stage
FROM newrelic/infrastructure-bundle:latest

# Backup original binary if it exists
RUN if [ -f /var/db/newrelic-infra/newrelic-integrations/bin/nri-mysql ]; then \
    mv /var/db/newrelic-infra/newrelic-integrations/bin/nri-mysql /var/db/newrelic-infra/newrelic-integrations/bin/nri-mysql.bak; \
    fi

# Copy the compiled binary from builder
COPY --from=builder /go/src/github.com/newrelic/nri-mysql/bin/nri-mysql /var/db/newrelic-infra/newrelic-integrations/bin/

# Set correct permissions
RUN chmod 755 /var/db/newrelic-infra/newrelic-integrations/bin/nri-mysql

# Create required directories
RUN mkdir -p /etc/newrelic-infra/integrations.d

# Verify binary works
RUN /var/db/newrelic-infra/newrelic-integrations/bin/nri-mysql -show_version

ENTRYPOINT ["/usr/bin/newrelic-infra"]
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\infrastructure\newrelic\mysql-config.yml ######
integrations:
  - name: nri-mysql
    interval: 15s
    command: all_data
    env:
      HOSTNAME: ${MYSQL_HOST}
      PORT: ${MYSQL_PORT}
      USERNAME: ${MYSQL_MONITOR_USER}
      PASSWORD: ${MYSQL_MONITOR_PASSWORD}
      REMOTE_MONITORING: true
      EXTENDED_METRICS: true
      EXTENDED_INNODB_METRICS: true
      EXTENDED_PERFORMANCE_METRICS: true
      TABLES_METRICS: true
    
    config:
      # Query performance monitoring
      slow_query_metrics: true
      query_response_time_stats: true
      
      # InnoDB metrics
      innodb_metrics:
        buffer_pool_metrics: true
        lock_metrics: true
        transaction_metrics: true
        page_metrics: true
        
      # Table metrics
      table_metrics:
        size_metrics: true
        index_metrics: true
        
      # Specific table monitoring
      tables:
        - departments
        - employees
        - salaries
        - dept_emp

      # Custom query monitoring
      custom_metrics:
        # Monitor employee distribution
        - query: |
            SELECT salary_tier, COUNT(*) as count
            FROM employees
            GROUP BY salary_tier
          metrics:
            - count
          description: "Employee count by salary tier"
        
        # Monitor salary trends
        - query: |
            SELECT AVG(salary) as avg_salary
            FROM salaries
            WHERE to_date = '9999-01-01'
          metrics:
            - avg_salary
          description: "Current average salary"
        
        # Lock contention monitoring
        - query: |
            SELECT COUNT(*) as locks
            FROM performance_schema.events_waits_current
            WHERE EVENT_NAME LIKE 'wait/lock%'
          metrics:
            - locks
          description: "Current lock count"

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\infrastructure\newrelic\newrelic-infra.yml ######
license_key: ${NEW_RELIC_LICENSE_KEY}
display_name: MySQL-Performance-Demo

custom_attributes:
  environment: performance_testing
  service: mysql_performance
  team: database_performance

log:
  file: /var/log/newrelic-infra/newrelic-infra.log
  level: info
  to_stdout: true

# Infrastructure monitoring settings
enable_process_metrics: true
strip_command_line: false

# Docker monitoring
docker_enabled: true
docker_use_daemon_socket: true

# Network monitoring
network_metrics_enabled: true
network_interface_filters:
  include:
    - eth0

# Storage monitoring
storage_metrics_enabled: true
storage_sample_rate: 20s

integrations:
  - name: nri-mysql
    interval: 15s
    timeout: 30s

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\infrastructure\newrelic\setup.sh ######
#!/bin/bash
mkdir -p /var/log/newrelic-infra
chown -R 1:1 /var/log/newrelic-infra
chmod -R 755 /var/log/newrelic-infra

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\infrastructure\newrelic\dashboards\mysql-performance.json ######
{
  "name": "MySQL Performance Dashboard",
  "description": "Comprehensive MySQL performance monitoring",
  "permissions": "PUBLIC_READ_WRITE",
  "pages": [
    {
      "name": "Overview",
      "widgets": [
        {
          "title": "Database Throughput",
          "visualization": "billboard",
          "nrql": "SELECT rate(count(*), 1 minute) FROM Transaction WHERE databaseCallCount > 0"
        },
        {
          "title": "Slow Queries",
          "visualization": "line_chart",
          "nrql": "SELECT count(*) FROM DatabaseQuery WHERE duration > 1000 TIMESERIES"
        },
        {
          "title": "Lock Contention",
          "visualization": "area_chart",
          "nrql": "SELECT average(locks) FROM CustomMetric WHERE metricName = 'current_lock_count' TIMESERIES"
        },
        {
          "title": "Memory Usage",
          "visualization": "area_chart",
          "nrql": "SELECT average(memoryUsedBytes) FROM MysqlSample TIMESERIES"
        }
      ]
    },
    {
      "name": "Query Analysis",
      "widgets": [
        {
          "title": "Top Time-Consuming Queries",
          "visualization": "table",
          "nrql": "SELECT average(duration) FROM DatabaseQuery FACET query LIMIT 10"
        },
        {
          "title": "Query Patterns",
          "visualization": "pie_chart",
          "nrql": "SELECT count(*) FROM DatabaseQuery FACET category LIMIT 10"
        }
      ]
    }
  ]
}

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\infrastructure\newrelic\integrations.d\mysql-config.yml ######
# infrastructure/newrelic/integrations.d/mysql-config.yml
integration_name: com.newrelic.mysql

instances:
  - name: mysql-main
    command: all_data
    arguments:
      username: ${MYSQL_MONITOR_USER}
      password: ${MYSQL_MONITOR_PASSWORD}
      hostname: mysql
      port: 3306

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\monitoring\newrelic\config.yml ######
integrations:
  - name: nri-mysql
    interval: 15s
    command: all_data
    env:
      HOSTNAME: ${MYSQL_HOST}
      PORT: ${MYSQL_PORT}
      USERNAME: ${MYSQL_MONITOR_USER}
      PASSWORD: ${MYSQL_MONITOR_PASSWORD}
      REMOTE_MONITORING: true
      EXTENDED_METRICS: true
      EXTENDED_INNODB_METRICS: true
      EXTENDED_PERFORMANCE_METRICS: true
      TABLES_METRICS: true
    
    config:
      slow_query_metrics: true
      query_response_time_stats: true
      
      innodb_metrics:
        buffer_pool_metrics: true
        lock_metrics: true
        page_metrics: true
        
      table_metrics:
        size_metrics: true
        index_metrics: true
        
      tables:
        - employees
        - departments
        - dept_emp
        - salaries

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\mysql\db-setup\Dockerfile ######
# Use an official MySQL runtime as a parent image
FROM mysql:8.0

# Set environment variables
ENV MYSQL_ROOT_PASSWORD=root_password

# Copy Flyway migrations
COPY db/migrations/ /flyway/sql/

# Copy scripts
COPY db/scripts/ /scripts/

# Copy configs
COPY db/config/mysql.cnf /etc/mysql/conf.d/

# Install Flyway
RUN apt-get update && apt-get install -y flyway

# Set up entrypoint
ENTRYPOINT ["/scripts/init-db.sh"]

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\mysql\db-setup\scripts\init-db.sh ######
#!/bin/bash
set -e

# Wait for MySQL to be ready
while ! mysqladmin ping -h"localhost" -u"root" -p"${MYSQL_ROOT_PASSWORD}" --silent; do
    echo "Waiting for MySQL to be ready..."
    sleep 2
done

# Run Flyway migrations in order
echo "Running Flyway migrations..."
for sql_file in /flyway/sql/V*__*.sql; do
    echo "Executing $sql_file..."
    mysql -u root -p"${MYSQL_ROOT_PASSWORD}" < "$sql_file"
done

# Load initial data
echo "Loading department data..."
cd /scripts && python3 departments_data.py

echo "Loading employee data..."
cd /scripts && python3 load_data.py

echo "Database initialization completed!"

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\scripts\deploy-local.sh ######
#!/bin/bash
set -e
echo "[INFO] Deploying local environment..."
docker-compose down --remove-orphans
docker-compose up -d --build
echo "[INFO] Environment deployed!"
docker-compose ps

# Call verify_environment.sh
bash verify_environment.sh

# Call verify_environment.sh
bash verify_environment.sh

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\scripts\generate_test_data.py ######
#!/usr/bin/env python3
import mysql.connector
import random
import argparse
from datetime import datetime, timedelta
import os

def create_salary_variations(conn, cursor):
    """Create salary variations for performance testing"""
    cursor.execute("""
        UPDATE salaries s
        JOIN employees e ON s.emp_no = e.emp_no
        SET s.salary = 
            CASE 
                WHEN e.salary_tier = 1 THEN s.salary * 1.5
                WHEN e.salary_tier = 2 THEN s.salary * 1.2
                ELSE s.salary
            END
        WHERE s.to_date = '9999-01-01'
    """)
    conn.commit()

def create_department_transfers(conn, cursor):
    """Create department transfer history"""
    cursor.execute("""
        INSERT INTO dept_emp (emp_no, dept_no, from_date, to_date)
        SELECT 
            e.emp_no,
            d.dept_no,
            DATE_SUB(CURRENT_DATE, INTERVAL FLOOR(RAND() * 365) DAY),
            '9999-01-01'
        FROM employees e
        CROSS JOIN departments d
        WHERE RAND() < 0.1
        AND NOT EXISTS (
            SELECT 1 FROM dept_emp de 
            WHERE de.emp_no = e.emp_no 
            AND de.dept_no = d.dept_no
        )
        LIMIT 1000
    """)
    conn.commit()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--variations', choices=['salaries', 'transfers', 'all'], 
                       default='all', help='Type of variations to generate')
    
    args = parser.parse_args()
    
    conn = mysql.connector.connect(
        host=os.getenv('MYSQL_HOST'),
        user=os.getenv('MYSQL_USER'),
        password=os.getenv('MYSQL_PASSWORD'),
        database=os.getenv('MYSQL_DATABASE')
    )
    
    cursor = conn.cursor()
    
    try:
        if args.variations in ['salaries', 'all']:
            print("Generating salary variations...")
            create_salary_variations(conn, cursor)
            
        if args.variations in ['transfers', 'all']:
            print("Generating department transfers...")
            create_department_transfers(conn, cursor)
            
    finally:
        cursor.close()
        conn.close()

if __name__ == "__main__":
    main()

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\scripts\run.sh ######
#!/bin/bash

MODE=${1:-docker}

case $MODE in
  docker)
    echo "Running with Docker MySQL..."
    cp .env.docker .env
    docker compose --profile docker up -d
    ;;
  local)
    echo "Running with local MySQL..."
    if [ ! -f .env.local ]; then
      echo "Error: .env.local not found. Run setup-local.sh first"
      exit 1
    fi
    cp .env.local .env
    docker compose up -d api load-generator
    ;;
  *)
    echo "Usage: $0 [docker|local]"
    exit 1
    ;;
esac

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\scripts\setup-local.sh ######
#!/bin/bash
set -e

echo "Setting up local MySQL environment..."

# Check MySQL installation
if ! command -v mysql &> /dev/null; then
    echo "Error: MySQL is not installed locally"
    exit 1
fi

# Create .env.local from template
if [ ! -f .env.local ]; then
    echo "Creating .env.local from template..."
    cp .env.local.template .env.local
    echo "Please update .env.local with your local MySQL credentials"
    exit 1
fi

# Source environment variables
source .env.local

# Create database and users
mysql -u root -p <<EOF
CREATE DATABASE IF NOT EXISTS ${MYSQL_DATABASE};
CREATE USER IF NOT EXISTS '${MYSQL_USER}'@'%' IDENTIFIED BY '${MYSQL_PASSWORD}';
GRANT ALL PRIVILEGES ON ${MYSQL_DATABASE}.* TO '${MYSQL_USER}'@'%';
CREATE USER IF NOT EXISTS '${MYSQL_MONITOR_USER}'@'%' IDENTIFIED BY '${MYSQL_MONITOR_PASSWORD}';
GRANT SELECT, PROCESS, REPLICATION CLIENT ON *.* TO '${MYSQL_MONITOR_USER}'@'%';
FLUSH PRIVILEGES;
EOF

# Initialize database
echo "Running initialization scripts..."
mysql -u ${MYSQL_USER} -p${MYSQL_PASSWORD} ${MYSQL_DATABASE} < db-setup/migrations/01_init_schema.sql

echo "Local MySQL setup completed!"

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\database.js ######
const mysql = require('mysql2/promise');

const pool = mysql.createPool({
  host: process.env.MYSQL_HOST,
  port: parseInt(process.env.MYSQL_PORT || '3306'),
  user: process.env.MYSQL_USER,
  password: process.env.MYSQL_PASSWORD,
  database: process.env.MYSQL_DATABASE,
  waitForConnections: true,
  connectionLimit: 10,
  queueLimit: 0,
  enableKeepAlive: true,
  keepAliveInitialDelay: 0,
  connectTimeout: 10000,
  acquireTimeout: 10000,
  debug: process.env.NODE_ENV === 'development',
  timezone: 'Z'
});

module.exports = pool;

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\Dockerfile ######
FROM node:20-alpine

# Install curl for healthcheck
RUN apk --no-cache add curl

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci --only=production

# Copy application files
COPY . .

# Create log directory
RUN mkdir -p /var/log/app

# Environment variables
ENV NODE_ENV=production \
    PORT=3000

# Healthcheck
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:3000/health || exit 1

# Expose port
EXPOSE 3000

# Start application
CMD ["node", "server.js"]
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\newrelic.cjs ######
// Renamed to newrelic.cjs
'use strict'

exports.config = {
  app_name: [process.env.NEW_RELIC_APP_NAME || 'MySQL-Performance-Demo-API'],
  license_key: process.env.NEW_RELIC_LICENSE_KEY,
  logging: {
    level: 'info',
    filepath: '/var/log/newrelic/newrelic_agent.log'
  },
  allow_all_headers: true,
  distributed_tracing: {
    enabled: true
  },
  transaction_tracer: {
    enabled: true,
    record_sql: 'raw',
    explain_threshold: 500
  },
  slow_sql: {
    enabled: true
  }
}

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\package.json ######
{
  "name": "mysql-perf-api",
  "version": "1.0.0",
  "type": "commonjs",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "NODE_ENV=development nodemon server.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "express-async-handler": "^1.2.0",
    "mysql2": "^3.6.5",
    "newrelic": "^11.10.0",
    "winston": "^3.11.0"
  },
  "devDependencies": {
    "nodemon": "^3.0.2"
  }
}

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\server.js ######
'use strict';

require('newrelic');
const express = require('express');
const asyncHandler = require('express-async-handler');

// Import utils
const { pool } = require('./src/utils/database');
const logger = require('./src/utils/logger');

// Import routes
const enterpriseRoutes = require('./src/routes/enterprise');
const healthRoutes = require('./src/routes/health');

// Import middleware
const errorHandler = require('./src/middleware/errorHandler');

// Create Express app
const app = express();

// Request logging middleware
app.use((req, res, next) => {
    const requestStart = Date.now();
    const requestId = Math.random().toString(36).substring(7);
    
    logger.info('Request started', {
        requestId,
        method: req.method,
        url: req.url
    });

    res.on('finish', () => {
        logger.info('Request completed', {
            requestId,
            method: req.method,
            url: req.url,
            statusCode: res.statusCode,
            duration: Date.now() - requestStart
        });
    });

    next();
});

// Database connection pool
app.locals.pool = pool;

// Use JSON middleware
app.use(express.json());

// Health check endpoint
app.use('/health', healthRoutes);

// Enterprise routes
app.use('/enterprise', enterpriseRoutes);

// Error handler
app.use(errorHandler);

// Start server
const PORT = process.env.API_PORT || 3000;
app.listen(PORT, () => {
    logger.info(`Server running on port ${PORT}`);
});

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\database\schema.sql ######
-- Create employees table
CREATE TABLE employees (
    emp_no INT PRIMARY KEY,
    first_name VARCHAR(255) NOT NULL,
    last_name VARCHAR(255) NOT NULL,
    gender ENUM('M', 'F') NOT NULL,
    birth_date DATE NOT NULL,
    hire_date DATE NOT NULL,
    salary_tier INT NOT NULL
);

-- Create departments table
CREATE TABLE departments (
    dept_no CHAR(4) PRIMARY KEY,
    dept_name VARCHAR(255) NOT NULL
);

-- Create dept_emp table
CREATE TABLE dept_emp (
    emp_no INT NOT NULL,
    dept_no CHAR(4) NOT NULL,
    from_date DATE NOT NULL,
    to_date DATE NOT NULL,
    PRIMARY KEY (emp_no, dept_no),
    FOREIGN KEY (emp_no) REFERENCES employees(emp_no),
    FOREIGN KEY (dept_no) REFERENCES departments(dept_no)
);

-- Create salaries table
CREATE TABLE salaries (
    emp_no INT NOT NULL,
    salary DECIMAL(10, 2) NOT NULL,
    from_date DATE NOT NULL,
    to_date DATE NOT NULL,
    PRIMARY KEY (emp_no, from_date),
    FOREIGN KEY (emp_no) REFERENCES employees(emp_no)
);

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\src\middleware\errorHandler.js ######
const logger = require('../utils/logger');

const errorHandler = (err, req, res, next) => {
    logger.error('Error occurred', {
        message: err.message,
        stack: err.stack,
        url: req.url,
        method: req.method,
    });

    res.status(500).json({ status: 'error', message: 'Internal server error' });
};

module.exports = errorHandler;

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\src\routes\enterprise.js ######
const express = require('express');
const router = express.Router();
const asyncHandler = require('express-async-handler');

// Import utils
const { executeQuery } = require('../utils/database');
const logger = require('../utils/logger');

// HR Department APIs
router.get('/hr/employee-analytics', asyncHandler(async (req, res) => {
    const results = await executeQuery(`
        SELECT 
            e.hire_year,
            d.dept_name,
            COUNT(DISTINCT e.emp_no) as employee_count,
            ROUND(AVG(s.salary), 2) as avg_salary,
            COUNT(DISTINCT CASE WHEN s.salary > 75000 THEN e.emp_no END) as high_earners
        FROM employees e
        JOIN dept_emp de ON e.emp_no = de.emp_no AND de.to_date = '9999-01-01'
        JOIN departments d ON de.dept_no = d.dept_no
        JOIN salaries s ON e.emp_no = s.emp_no AND s.to_date = '9999-01-01'
        GROUP BY e.hire_year, d.dept_name
        ORDER BY e.hire_year DESC, d.dept_name
    `, [], 'HR Analytics');
    
    res.json({ status: 'success', data: results });
}));

// Department transfer with lock demonstration
router.post('/hr/department-transfer', asyncHandler(async (req, res) => {
    const { empNo, newDeptNo, salary } = req.body;
    const conn = await req.app.locals.pool.getConnection();
    
    try {
        await conn.beginTransaction();
        
        // Update existing department assignment
        await conn.execute(`
            UPDATE dept_emp 
            SET to_date = CURRENT_DATE()
            WHERE emp_no = ? AND to_date = '9999-01-01'
        `, [empNo]);
        
        // Insert new department assignment
        await conn.execute(`
            INSERT INTO dept_emp (emp_no, dept_no, from_date, to_date)
            VALUES (?, ?, CURRENT_DATE(), '9999-01-01')
        `, [empNo, newDeptNo]);
        
        // Update salary if provided
        if (salary) {
            await conn.execute(`
                UPDATE salaries 
                SET to_date = CURRENT_DATE()
                WHERE emp_no = ? AND to_date = '9999-01-01'
            `, [empNo]);
            
            await conn.execute(`
                INSERT INTO salaries (emp_no, salary, from_date, to_date)
                VALUES (?, ?, CURRENT_DATE(), '9999-01-01')
            `, [empNo, salary]);
        }
        
        // Simulate processing time for lock demonstration
        await conn.execute('DO SLEEP(1)');
        
        await conn.commit();
        res.json({ status: 'success', message: 'Transfer processed' });
    } catch (error) {
        await conn.rollback();
        throw error;
    } finally {
        conn.release();
    }
}));

// Finance compliance audit
router.get('/finance/audit', asyncHandler(async (req, res) => {
    const { year, quarter } = req.query;
    
    const results = await executeQuery(`
        WITH salary_changes AS (
            SELECT 
                e.emp_no,
                d.dept_name,
                s1.salary as old_salary,
                s2.salary as new_salary,
                s2.from_date as change_date,
                ROUND((s2.salary - s1.salary) / s1.salary * 100, 2) as change_percentage
            FROM employees e
            JOIN dept_emp de ON e.emp_no = de.emp_no 
            JOIN departments d ON de.dept_no = d.dept_no
            JOIN salaries s1 ON e.emp_no = s1.emp_no
            JOIN salaries s2 ON e.emp_no = s2.emp_no
                AND s2.from_date > s1.from_date
                AND YEAR(s2.from_date) = ?
                AND QUARTER(s2.from_date) = ?
            WHERE de.to_date = '9999-01-01'
        )
        SELECT 
            dept_name,
            COUNT(*) as total_changes,
            ROUND(AVG(change_percentage), 2) as avg_increase
        FROM salary_changes
        GROUP BY dept_name
        ORDER BY avg_increase DESC
    `, [year || new Date().getFullYear(), quarter || 1], 'Finance Audit');
    
    res.json({ status: 'success', data: results });
}));

// Data science analysis
router.get('/analytics/retention', asyncHandler(async (req, res) => {
    const results = await executeQuery(`
        WITH employee_metrics AS (
            SELECT 
                e.emp_no,
                e.hire_year,
                e.salary_tier,
                COUNT(DISTINCT d.dept_no) as dept_changes,
                COUNT(DISTINCT s.salary) as salary_changes,
                DATEDIFF(COALESCE(MIN(CASE WHEN de.to_date != '9999-01-01' 
                    THEN de.to_date END), CURRENT_DATE), e.hire_date) as tenure_days
            FROM employees e
            LEFT JOIN dept_emp de ON e.emp_no = de.emp_no
            LEFT JOIN departments d ON de.dept_no = d.dept_no
            LEFT JOIN salaries s ON e.emp_no = s.emp_no
            GROUP BY e.emp_no, e.hire_year, e.salary_tier
        )
        SELECT 
            hire_year,
            salary_tier,
            COUNT(*) as employee_count,
            ROUND(AVG(dept_changes), 2) as avg_dept_changes,
            ROUND(AVG(tenure_days) / 365, 2) as avg_tenure_years
        FROM employee_metrics
        GROUP BY hire_year, salary_tier
        ORDER BY hire_year DESC, salary_tier
    `, [], 'Retention Analysis');
    
    res.json({ status: 'success', data: results });
}));

// Department dashboard
router.get('/dashboard/metrics', asyncHandler(async (req, res) => {
    const { deptNo } = req.query;
    
    const results = await executeQuery(`
        SELECT 
            d.dept_name,
            COUNT(DISTINCT de.emp_no) as current_employees,
            ROUND(AVG(s.salary), 2) as avg_salary,
            COUNT(DISTINCT CASE 
                WHEN DATEDIFF(CURRENT_DATE, de.from_date) <= 90 
                THEN de.emp_no 
            END) as recent_transfers,
            d.manager_budget,
            ROUND(SUM(s.salary) / d.manager_budget * 100, 2) as budget_utilization
        FROM departments d
        LEFT JOIN dept_emp de ON d.dept_no = de.dept_no AND de.to_date = '9999-01-01'
        LEFT JOIN salaries s ON de.emp_no = s.emp_no AND s.to_date = '9999-01-01'
        WHERE d.dept_no = ?
        GROUP BY d.dept_name, d.manager_budget
    `, [deptNo], 'Department Dashboard');
    
    res.json({ status: 'success', data: results });
}));

module.exports = router;

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\src\routes\health.js ######
const express = require('express');
const router = express.Router();

router.get('/', (req, res) => {
    res.json({ status: 'ok', message: 'API is healthy' });
});

module.exports = router;

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\src\utils\database.js ######
const mysql = require('mysql2/promise');
const logger = require('./logger');

const pool = mysql.createPool({
    host: process.env.MYSQL_HOST,
    port: parseInt(process.env.MYSQL_PORT || '3306'),
    user: process.env.MYSQL_USER,
    password: process.env.MYSQL_PASSWORD,
    database: process.env.MYSQL_DATABASE,
    waitForConnections: true,
    connectionLimit: 10,
    queueLimit: 0,
    connectTimeout: 10000,
    timezone: 'Z'
});

const executeQuery = async (query, params = [], context = '') => {
    const start = process.hrtime();
    try {
        const [results] = await pool.execute(query, params);
        const elapsed = process.hrtime(start);
        const duration = (elapsed[0] * 1000) + (elapsed[1] / 1000000);
        
        logger.info('Query executed', {
            context,
            duration,
            rowCount: results.length
        });
        
        return results;
    } catch (error) {
        logger.error('Query failed', {
            context,
            error: error.message,
            query: query.substring(0, 200)
        });
        throw error;
    }
};

module.exports = {
    pool,
    executeQuery
};

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\src\utils\logger.js ######
const winston = require('winston');

const logger = winston.createLogger({
    level: 'info',
    format: winston.format.combine(
        winston.format.timestamp(),
        winston.format.json()
    ),
    transports: [
        new winston.transports.Console(),
    ],
});

module.exports = logger;

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\load-generator\Dockerfile ######
FROM grafana/k6:latest

# Copy test scripts
COPY scripts/ /scripts/

WORKDIR /scripts

ENV API_URL=http://api:3000 \
    K6_VUS=50 \
    K6_DURATION=30m

# Run load test
CMD ["run", "load-test.js"]
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\load-generator\scripts\custom-metrics.js ######
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate } from 'k6/metrics';

// Custom metrics
const errorRate = new Rate('error_rate');
const queryLatency = new Trend('query_latency');

// NewRelic custom events
const newrelicEvents = {
  apiEndpoint: 'https://insights-collector.newrelic.com/v1/accounts/${ACCOUNT_ID}/events',
  licenseKey: __ENV.NEW_RELIC_LICENSE_KEY,
  
  recordEvent(eventType, attributes) {
    const payload = {
      eventType,
      timestamp: Date.now(),
      ...attributes
    };
    
    http.post(
      this.apiEndpoint,
      JSON.stringify([payload]),
      {
        headers: {
          'X-Insert-Key': this.licenseKey,
          'Content-Type': 'application/json'
        }
      }
    );
  }
};

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\load-generator\scripts\load-test.js ######
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend } from 'k6/metrics';

// Custom metrics
const errorRate = new Rate('error_rate');
const successRate = new Rate('success_rate');
const queryLatency = new Trend('query_latency');
const requestsPerSecond = new Rate('requests_per_second');

// Test configuration
export const options = {
    stages: [
        { duration: '2m', target: 50 },  // Ramp up
        { duration: '25m', target: 50 }, // Stay at peak load
        { duration: '3m', target: 0 }    // Ramp down
    ],
    thresholds: {
        http_req_duration: ['p(95)<2000'], // 95% of requests under 2s
        'error_rate': ['rate<0.1'],        // Error rate under 10%
        'success_rate': ['rate>0.9'],      // Success rate over 90%
        'requests_per_second': ['rate>30']  // At least 30 RPS
    }
};

const API_BASE = __ENV.API_URL || 'http://api:3000';

// Endpoint definitions with weights
const ENDPOINTS = [
    { path: '/random_search', weight: 3 },
    { path: '/aggregation', weight: 2 },
    { path: '/complex_join', weight: 3 },
    { path: '/health', weight: 1 }
];

// Endpoint selection based on weights
function selectEndpoint() {
    const totalWeight = ENDPOINTS.reduce((sum, ep) => sum + ep.weight, 0);
    let random = Math.random() * totalWeight;
    
    for (const endpoint of ENDPOINTS) {
        if (random < endpoint.weight) return endpoint.path;
        random -= endpoint.weight;
    }
    return ENDPOINTS[0].path;
}

// VU script
export default function() {
    const endpoint = selectEndpoint();
    const url = `${API_BASE}${endpoint}`;
    const start = new Date();
    
    const response = http.get(url);
    const duration = new Date() - start;
    
    // Record metrics
    queryLatency.add(duration);
    errorRate.add(response.status !== 200);
    successRate.add(response.status === 200);
    requestsPerSecond.add(1);
    
    // Detailed logging for sample requests
    if (Math.random() < 0.01) {
        console.log(JSON.stringify({
            timestamp: new Date().toISOString(),
            endpoint,
            status: response.status,
            duration,
            body: response.body.slice(0, 100) + '...'
        }));
    }
    
    // Response validation
    check(response, {
        'status is 200': (r) => r.status === 200,
        'response has data': (r) => {
            try {
                const body = JSON.parse(r.body);
                return body && body.rows && Array.isArray(body.rows);
            } catch {
                return false;
            }
        }
    });
    
    // Random sleep between requests (0.5-1.5s)
    sleep(Math.random() + 0.5);
}

// Test lifecycle hooks
export function setup() {
    console.log('Load test starting...');
    // Verify API health before starting
    const healthCheck = http.get(`${API_BASE}/health`);
    check(healthCheck, {
        'API is healthy': (r) => r.status === 200
    });
}

export function teardown(data) {
    console.log('Load test completed');
    console.log('Final metrics:', {
        errorRate: errorRate.value,
        successRate: successRate.value,
        avgLatency: queryLatency.avg,
        p95Latency: queryLatency.p(95)
    });
}
