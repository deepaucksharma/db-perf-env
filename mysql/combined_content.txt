###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env ######
# Database Configuration
MYSQL_ROOT_PASSWORD=YourSecurePassword123
MYSQL_DATABASE=employees
MYSQL_USER=myuser
MYSQL_PASSWORD=userpass123

# New Relic MySQL Monitor User
MYSQL_MONITOR_USER=newrelic
MYSQL_MONITOR_PASSWORD=newrelicpass123

# API Configuration
API_PORT=3000

# Data Generation
BATCH_SIZE=1000
TOTAL_EMPLOYEES=10000

# Performance Testing
ENABLE_SLOW_QUERIES=true
SLOW_QUERY_TIME=1

# Load Generator Configuration
K6_VUS=50
K6_DURATION=30m

# New Relic Configuration
NEW_RELIC_LICENSE_KEY=eu01xx0ce15fea0f04db4b9d9ecce89dFFFFNRAL
NEW_RELIC_APP_NAME=MySQL-Employees-Performance-Demo

# Environment
NODE_ENV=production
MYSQL_INTEGRATION_BRANCH=master

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env.example ######
MYSQL_ROOT_PASSWORD=demo123
MYSQL_DATABASE=employees
MYSQL_USER=myuser
MYSQL_PASSWORD=demopass

# New Relic MySQL Monitor User
MYSQL_MONITOR_USER=newrelic
MYSQL_MONITOR_PASSWORD=nrpass123

# API Configuration
API_PORT=3000

# Data Generation
BATCH_SIZE=5000
TOTAL_EMPLOYEES=100000

# Performance Testing
ENABLE_SLOW_QUERIES=true
SLOW_QUERY_TIME=0.5

# Load Generator Configuration
K6_VUS=100
K6_DURATION=30m

# New Relic Configuration
NEW_RELIC_LICENSE_KEY=your_license_key
NEW_RELIC_APP_NAME=MySQL-Demo

# Environment
NODE_ENV=development

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.gitignore ######
# Environment files
.env*
!.env.example

# Logs
*.log
logs/
mysql_logs/

# Dependencies
node_modules/
__pycache__/
*.pyc

# IDE and editors
.vscode/
.idea/
*.swp
*.swo
*~

# Build and data directories
mysql_data/
data/
dist/
build/

# Docker
.docker/
docker-compose.override.yml

# Temporary files
*.tmp
*.temp
.DS_Store

# Debug logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\AzureDeploy.sh ######

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\combined_content.txt ######
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env ######
# Database Configuration
MYSQL_ROOT_PASSWORD=YourSecurePassword123
MYSQL_DATABASE=employees
MYSQL_USER=myuser
MYSQL_PASSWORD=userpass123

# New Relic MySQL Monitor User
MYSQL_MONITOR_USER=newrelic
MYSQL_MONITOR_PASSWORD=newrelicpass123

# API Configuration
API_PORT=3000

# Data Generation
BATCH_SIZE=1000
TOTAL_EMPLOYEES=10000

# Performance Testing
ENABLE_SLOW_QUERIES=true
SLOW_QUERY_TIME=1

# Load Generator Configuration
K6_VUS=50
K6_DURATION=30m

# New Relic Configuration
NEW_RELIC_LICENSE_KEY=eu01xx0ce15fea0f04db4b9d9ecce89dFFFFNRAL
NEW_RELIC_APP_NAME=MySQL-Employees-Performance-Demo

# Environment
NODE_ENV=production
MYSQL_INTEGRATION_BRANCH=master

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.env.example ######
MYSQL_ROOT_PASSWORD=demo123
MYSQL_DATABASE=employees
MYSQL_USER=myuser
MYSQL_PASSWORD=demopass

# New Relic MySQL Monitor User
MYSQL_MONITOR_USER=newrelic
MYSQL_MONITOR_PASSWORD=nrpass123

# API Configuration
API_PORT=3000

# Data Generation
BATCH_SIZE=5000
TOTAL_EMPLOYEES=100000

# Performance Testing
ENABLE_SLOW_QUERIES=true
SLOW_QUERY_TIME=0.5

# Load Generator Configuration
K6_VUS=100
K6_DURATION=30m

# New Relic Configuration
NEW_RELIC_LICENSE_KEY=your_license_key
NEW_RELIC_APP_NAME=MySQL-Demo

# Environment
NODE_ENV=development

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\.gitignore ######
# Environment files
.env*
!.env.example

# Logs
*.log
logs/
mysql_logs/

# Dependencies
node_modules/
__pycache__/
*.pyc

# IDE and editors
.vscode/
.idea/
*.swp
*.swo
*~

# Build and data directories
mysql_data/
data/
dist/
build/

# Docker
.docker/
docker-compose.override.yml

# Temporary files
*.tmp
*.temp
.DS_Store

# Debug logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\AzureDeploy.sh ######


###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\docker-compose.yml ######
version: '3.8'

services:
  mysql:
    build:
      context: ./db-setup
      dockerfile: Dockerfile
    container_name: mysql-db
    environment:
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
      - MYSQL_DATABASE=${MYSQL_DATABASE}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      - MYSQL_MONITOR_USER=${MYSQL_MONITOR_USER}
      - MYSQL_MONITOR_PASSWORD=${MYSQL_MONITOR_PASSWORD}
      - BATCH_SIZE=${BATCH_SIZE}
      - TOTAL_EMPLOYEES=${TOTAL_EMPLOYEES}
      - TZ=UTC
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - mysql_logs:/var/log/mysql
    networks:
      - backend
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  api:
    build:
      context: ./services/api
      dockerfile: Dockerfile
    container_name: employees-api
    environment:
      - MYSQL_HOST=mysql-db
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      - MYSQL_DATABASE=${MYSQL_DATABASE}
      - API_PORT=${API_PORT}
      - NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
      - NEW_RELIC_APP_NAME=${NEW_RELIC_APP_NAME}
    ports:
      - "${API_PORT}:3000"
    depends_on:
      mysql:
        condition: service_healthy
    networks:
      - backend

  load-generator:
    build:
      context: ./services/load-generator
      dockerfile: Dockerfile
    environment:
      - API_URL=http://api:3000
      - K6_VUS=${K6_VUS}
      - K6_DURATION=${K6_DURATION}
    depends_on:
      - api
    networks:
      - backend

networks:
  backend:
    driver: bridge

volumes:
  mysql_data:
  mysql_logs:
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\config\mysql.cnf ######
# MySQL Configuration
[mysqld]
bind-address = 0.0.0.0
port = 3306
default-storage-engine = InnoDB
innodb_buffer_pool_size = 1G

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\config\performance-schema.cnf ######
# Performance Schema Configuration
[mysqld]
performance_schema = ON
performance_schema_instrument = 'statement/sql/*=ON'
performance_schema_consumer_events_statements_history = ON
performance_schema_consumer_events_statements_history_long = ON

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\Dockerfile ######
FROM mysql:8.0

# Install required utilities
RUN microdnf update && \
    microdnf install -y curl python3 python3-pip && \
    microdnf clean all

# Create required directories
RUN mkdir -p /var/log/mysql /scripts /docker-entrypoint-initdb.d && \
    chown mysql:mysql /var/log/mysql && \
    chmod 755 /var/log/mysql

# Copy configuration files
COPY config/ /etc/mysql/conf.d/
RUN chown -R mysql:mysql /etc/mysql/conf.d/ && \
    chmod 0444 /etc/mysql/conf.d/*.cnf

# Copy SQL initialization files
COPY migrations/V1__init_monitoring.sql \
     migrations/V2__create_base_schema.sql \
     migrations/V3__add_indexes.sql \
     migrations/V4__create_views.sql \
     /docker-entrypoint-initdb.d/

# Copy Python scripts and requirements
COPY scripts/ /scripts/
COPY requirements.txt /scripts/

# Install Python dependencies
RUN pip3 install --no-cache-dir -r /scripts/requirements.txt && \
    rm -rf /root/.cache/pip

# Create data initialization script
RUN echo '#!/bin/bash' > /docker-entrypoint-initdb.d/zz_init_data.sh && \
    echo 'set -e' >> /docker-entrypoint-initdb.d/zz_init_data.sh && \
    echo 'echo "Waiting for MySQL to be ready..."' >> /docker-entrypoint-initdb.d/zz_init_data.sh && \
    echo 'sleep 10' >> /docker-entrypoint-initdb.d/zz_init_data.sh && \
    echo 'echo "Starting data initialization..."' >> /docker-entrypoint-initdb.d/zz_init_data.sh && \
    echo 'python3 /scripts/create_tables.py' >> /docker-entrypoint-initdb.d/zz_init_data.sh && \
    echo 'python3 /scripts/departments_data.py' >> /docker-entrypoint-initdb.d/zz_init_data.sh && \
    echo 'python3 /scripts/load_data.py' >> /docker-entrypoint-initdb.d/zz_init_data.sh && \
    chmod +x /docker-entrypoint-initdb.d/zz_init_data.sh

# Create healthcheck script
RUN echo '#!/bin/bash' > /healthcheck.sh && \
    echo 'mysqladmin ping -h"localhost" -u"$MYSQL_USER" -p"$MYSQL_PASSWORD" --silent' >> /healthcheck.sh && \
    chmod +x /healthcheck.sh

EXPOSE 3306

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD ["/healthcheck.sh"]

CMD ["mysqld"]
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\requirements.txt ######
mysql-connector-python==8.0.33
Faker==15.3.4
newrelic==8.8.0
python-dotenv==1.0.0

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\config\mysql.cnf ######
[mysqld]
# InnoDB Settings
innodb_buffer_pool_size = 256M
innodb_buffer_pool_instances = 2
innodb_file_per_table = 1
innodb_flush_log_at_trx_commit = 2
innodb_log_buffer_size = 16M
innodb_io_capacity = 1000
innodb_flush_method = O_DIRECT
innodb_thread_concurrency = 0

# Buffer Settings
sort_buffer_size = 1M
read_buffer_size = 1M
join_buffer_size = 1M
tmp_table_size = 32M
max_heap_table_size = 32M

# Connection Settings
max_connections = 100
thread_cache_size = 50
wait_timeout = 28800
interactive_timeout = 28800

# Performance Schema
performance_schema = ON
performance_schema_max_digest_length = 4096
performance_schema_max_sql_text_length = 4096

# Slow Query Logging
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 0.5
log_queries_not_using_indexes = 1
log_slow_admin_statements = 1
min_examined_row_limit = 100
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\config\performance-schema.cnf ######
[mysqld]
# Enable Performance Schema
performance_schema = ON
performance_schema_max_digest_length = 4096
performance_schema_max_sql_text_length = 4096
performance_schema_max_memory_classes = 500

# Instrument Configuration
performance_schema_instrument = 'memory/%=ON'
performance_schema_instrument = 'statement/%=ON'
performance_schema_instrument = 'wait/lock/metadata/sql/mdl=ON'
performance_schema_instrument = 'wait/lock/table/sql/handler=ON'
performance_schema_instrument = 'table/%=ON'
performance_schema_instrument = 'wait/io/table/sql/handler=ON'

# Consumer Configuration
performance_schema_consumer_events_statements_current = ON
performance_schema_consumer_events_statements_history = ON
performance_schema_consumer_events_statements_history_long = ON
performance_schema_consumer_statements_digest = ON
performance_schema_consumer_thread_instrumentation = ON
performance_schema_consumer_global_instrumentation = ON
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\migrations\V1__init_monitoring.sql ######
SET @OLD_SQL_MODE = @@SQL_MODE;
SET SQL_MODE = 'NO_ENGINE_SUBSTITUTION';
SET NAMES utf8mb4;

-- Create users with basic permissions for demo
CREATE USER IF NOT EXISTS '${MYSQL_USER}'@'%' IDENTIFIED BY '${MYSQL_PASSWORD}';
CREATE USER IF NOT EXISTS '${MYSQL_MONITOR_USER}'@'%' IDENTIFIED BY '${MYSQL_MONITOR_PASSWORD}';

-- Grant permissions to application user
GRANT ALL PRIVILEGES ON ${MYSQL_DATABASE}.* TO '${MYSQL_USER}'@'%';

-- Grant monitoring permissions
GRANT SELECT, PROCESS, REPLICATION CLIENT ON *.* TO '${MYSQL_MONITOR_USER}'@'%';
GRANT SELECT ON performance_schema.* TO '${MYSQL_MONITOR_USER}'@'%';
GRANT SELECT ON sys.* TO '${MYSQL_MONITOR_USER}'@'%';
GRANT SELECT ON information_schema.* TO '${MYSQL_MONITOR_USER}'@'%';

-- Enable performance monitoring
UPDATE performance_schema.setup_instruments 
SET ENABLED = 'YES', TIMED = 'YES'
WHERE NAME LIKE '%statement/%' 
   OR NAME LIKE '%stage/%'
   OR NAME LIKE '%wait/%'
   OR NAME LIKE '%memory/%';

UPDATE performance_schema.setup_consumers
SET ENABLED = 'YES'
WHERE NAME LIKE '%events%';

FLUSH PRIVILEGES;
SET SQL_MODE = @OLD_SQL_MODE;
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\migrations\V2__create_base_schema.sql ######
CREATE DATABASE IF NOT EXISTS ${MYSQL_DATABASE};
USE ${MYSQL_DATABASE};

CREATE TABLE IF NOT EXISTS employees (
    emp_no INT PRIMARY KEY,
    birth_date DATE NOT NULL,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    gender ENUM('M','F') NOT NULL,
    hire_date DATE NOT NULL,
    birth_month INT GENERATED ALWAYS AS (MONTH(birth_date)) STORED,
    hire_year INT GENERATED ALWAYS AS (YEAR(hire_date)) STORED,
    salary_tier INT GENERATED ALWAYS AS (
        CASE 
            WHEN emp_no % 4 = 0 THEN 1
            WHEN emp_no % 4 = 1 THEN 2
            WHEN emp_no % 4 = 2 THEN 3
            ELSE 4
        END
    ) STORED,
    last_modified TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
) ENGINE=InnoDB;

CREATE TABLE IF NOT EXISTS departments (
    dept_no CHAR(4) PRIMARY KEY,
    dept_name VARCHAR(40) NOT NULL UNIQUE,
    manager_budget DECIMAL(15,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
) ENGINE=InnoDB;

CREATE TABLE IF NOT EXISTS salaries (
    id INT AUTO_INCREMENT PRIMARY KEY,
    emp_no INT NOT NULL,
    salary INT NOT NULL,
    from_date DATE NOT NULL,
    to_date DATE NOT NULL,
    FOREIGN KEY (emp_no) REFERENCES employees (emp_no) ON DELETE CASCADE
) ENGINE=InnoDB;

CREATE TABLE IF NOT EXISTS dept_emp (
    emp_no INT NOT NULL,
    dept_no CHAR(4) NOT NULL,
    from_date DATE NOT NULL,
    to_date DATE NOT NULL,
    PRIMARY KEY (emp_no, dept_no),
    FOREIGN KEY (emp_no) REFERENCES employees (emp_no) ON DELETE CASCADE,
    FOREIGN KEY (dept_no) REFERENCES departments (dept_no) ON DELETE CASCADE
) ENGINE=InnoDB;
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\migrations\V3__add_indexes.sql ######
USE ${MYSQL_DATABASE};

-- Employee indexes
CREATE INDEX idx_employees_gender ON employees(gender);
CREATE INDEX idx_employees_birth_month ON employees(birth_month);
CREATE INDEX idx_employees_hire_year ON employees(hire_year);
CREATE INDEX idx_emp_name1 ON employees(last_name, first_name);
CREATE INDEX idx_emp_name2 ON employees(first_name, last_name);
CREATE INDEX idx_emp_name3 ON employees(last_name, birth_date, first_name);
CREATE INDEX idx_salary_tier ON employees(salary_tier);

-- Salary indexes
CREATE INDEX idx_salaries_amount ON salaries(salary);
CREATE INDEX idx_salaries_dates ON salaries(from_date, to_date);
CREATE INDEX idx_salaries_emp_date ON salaries(emp_no, from_date);

-- Department indexes
CREATE INDEX idx_dept_emp_1 ON dept_emp(emp_no, dept_no, from_date);
CREATE INDEX idx_dept_emp_2 ON dept_emp(emp_no, from_date, dept_no);
CREATE INDEX idx_dept_emp_3 ON dept_emp(dept_no, emp_no, from_date);
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\migrations\V4__create_views.sql ######
USE ${MYSQL_DATABASE};

-- Employee statistics view
CREATE OR REPLACE VIEW employee_stats AS
SELECT 
    YEAR(birth_date) as birth_year,
    gender,
    COUNT(*) as total_count,
    AVG(MONTH(birth_date)) as avg_birth_month,
    SUM(CASE WHEN gender = 'M' THEN 1 ELSE 0 END) as male_count
FROM employees
GROUP BY YEAR(birth_date), gender;

-- Employee performance view with hierarchical data
CREATE OR REPLACE VIEW employee_performance AS
WITH RECURSIVE date_sequence AS (
    SELECT CURDATE() - INTERVAL 12 MONTH as date
    UNION ALL
    SELECT date + INTERVAL 1 MONTH 
    FROM date_sequence 
    WHERE date < CURDATE()
)
SELECT 
    d.dept_name,
    ds.date as month_date,
    COUNT(DISTINCT e.emp_no) as employee_count,
    AVG(s.salary) as avg_salary,
    SUM(s.salary) as total_salary,
    COUNT(DISTINCT de.dept_no) as dept_count
FROM 
    date_sequence ds
    CROSS JOIN departments d
    LEFT JOIN dept_emp de ON de.dept_no = d.dept_no
    LEFT JOIN employees e ON de.emp_no = e.emp_no
    LEFT JOIN salaries s ON e.emp_no = s.emp_no
        AND s.from_date <= ds.date 
        AND s.to_date >= ds.date
GROUP BY 
    d.dept_name,
    ds.date;
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\create_tables.py ######
import mysql.connector
import os
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_tables():
    db_config = {
        'host': os.getenv('MYSQL_HOST', 'localhost'),
        'user': os.getenv('MYSQL_USER', 'root'),
        'password': os.getenv('MYSQL_ROOT_PASSWORD', 'demo123'),
        'database': os.getenv('MYSQL_DATABASE', 'employees')
    }
    
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        
        # Create departments table if not exists
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS departments (
                dept_no CHAR(4) PRIMARY KEY,
                dept_name VARCHAR(40) NOT NULL,
                manager_budget DECIMAL(15,2),
                UNIQUE KEY uk_dept_name (dept_name)
            )
        """)
        
        # Create employees table if not exists
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS employees (
                emp_no INT PRIMARY KEY,
                birth_date DATE NOT NULL,
                first_name VARCHAR(50) NOT NULL,
                last_name VARCHAR(50) NOT NULL,
                gender ENUM('M','F') NOT NULL,
                hire_date DATE NOT NULL,
                birth_month INT GENERATED ALWAYS AS (MONTH(birth_date)) STORED,
                hire_year INT GENERATED ALWAYS AS (YEAR(hire_date)) STORED,
                salary_tier INT GENERATED ALWAYS AS (
                    CASE 
                        WHEN emp_no % 4 = 0 THEN 1
                        WHEN emp_no % 4 = 1 THEN 2
                        WHEN emp_no % 4 = 2 THEN 3
                        ELSE 4
                    END
                ) STORED
            )
        """)
        
        conn.commit()
        logger.info("Tables created successfully")
        
    except Exception as e:
        logger.error(f"Error creating tables: {str(e)}")
        conn.rollback()
        raise
    finally:
        if 'conn' in locals():
            cursor.close()
            conn.close()

if __name__ == "__main__":
    create_tables()
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\departments_data.py ######
import mysql.connector
import os
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def insert_departments():
    db_config = {
        'host': os.getenv('MYSQL_HOST', 'localhost'),
        'user': os.getenv('MYSQL_USER', 'root'),
        'password': os.getenv('MYSQL_ROOT_PASSWORD', 'demo123'),
        'database': os.getenv('MYSQL_DATABASE', 'employees')
    }
    
    departments = [
        ('d001', 'Marketing'),
        ('d002', 'Finance'),
        ('d003', 'Human Resources'),
        ('d004', 'Research and Development'),
        ('d005', 'Quality Assurance'),
        ('d006', 'Sales'),
        ('d007', 'IT'),
        ('d008', 'Operations'),
        ('d009', 'Customer Support'),
        ('d010', 'Product Management')
    ]
    
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        
        cursor.executemany("""
            INSERT INTO departments (dept_no, dept_name)
            VALUES (%s, %s)
            ON DUPLICATE KEY UPDATE dept_name = VALUES(dept_name)
        """, departments)
        
        # Initialize manager_budget randomly
        cursor.execute("""
            UPDATE departments 
            SET manager_budget = FLOOR(1000000 + RAND() * 1000000)
            WHERE manager_budget IS NULL
        """)
        
        conn.commit()
        logger.info(f"Inserted {len(departments)} departments successfully")
        
    except Exception as e:
        logger.error(f"Error inserting departments: {str(e)}")
        conn.rollback()
        raise
    finally:
        if 'conn' in locals():
            cursor.close()
            conn.close()

if __name__ == "__main__":
    insert_departments()
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\healthcheck.sh ######
#!/bin/bash
set -eo pipefail

MYSQL_USER=${MYSQL_HEALTHCHECK_USER:-$MYSQL_USER}
MYSQL_PASS=${MYSQL_HEALTHCHECK_PASSWORD:-$MYSQL_PASSWORD}

if ! mysqladmin ping -h"localhost" -u"$MYSQL_USER" -p"$MYSQL_PASS" --silent; then
    exit 1
fi

mysql -u"$MYSQL_USER" -p"$MYSQL_PASS" -e "SELECT 1;" >/dev/null 2>&1
exit $?

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\load_data.py ######
import mysql.connector
import random
from datetime import date, timedelta
from faker import Faker
import os
import logging
from concurrent.futures import ThreadPoolExecutor
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def generate_employee_batch(start_emp_no, batch_size, fake):
    employees = []
    salaries = []
    dept_assignments = []
    departments = ['d001', 'd002', 'd003', 'd004', 'd005', 'd006', 'd007', 'd008', 'd009', 'd010']
    
    for i in range(batch_size):
        emp_no = start_emp_no + i
        birth_date = fake.date_between(start_date='-65y', end_date='-25y')
        hire_date = fake.date_between(start_date='-20y', end_date='today')
        
        employees.append((
            emp_no,
            birth_date,
            fake.first_name(),
            fake.last_name(),
            random.choice(['M', 'F']),
            hire_date
        ))
        
        # Generate 2-4 salary records per employee
        current_date = hire_date
        for _ in range(random.randint(2, 4)):
            base_salary = random.randint(30000, 150000)
            # Add some outliers for interesting data
            if random.random() < 0.05:  # 5% chance
                base_salary *= random.uniform(1.5, 2.5)
            
            salaries.append((
                emp_no,
                int(base_salary),
                current_date,
                date(9999, 1, 1) if _ == 0 else current_date + timedelta(days=random.randint(365, 1095))
            ))
            current_date += timedelta(days=random.randint(365, 1095))
        
        # Assign to 1-2 departments
        num_departments = random.randint(1, 2)
        selected_departments = random.sample(departments, num_departments)
        for dept_no in selected_departments:
            dept_assignments.append((
                emp_no,
                dept_no,
                hire_date,
                date(9999, 1, 1)
            ))
    
    return employees, salaries, dept_assignments

def main():
    batch_size = int(os.getenv('BATCH_SIZE', 1000))
    total_employees = int(os.getenv('TOTAL_EMPLOYEES', 10000))
    
    db_config = {
        'host': os.getenv('MYSQL_HOST', 'localhost'),
        'user': os.getenv('MYSQL_USER', 'root'),
        'password': os.getenv('MYSQL_ROOT_PASSWORD', 'demo123'),
        'database': os.getenv('MYSQL_DATABASE', 'employees')
    }
    
    fake = Faker()
    
    logger.info(f"Starting data load: {total_employees} employees in batches of {batch_size}")
    
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        
        for batch_start in range(0, total_employees, batch_size):
            start_time = time.time()
            
            employees, salaries, dept_assignments = generate_employee_batch(
                batch_start + 1000000,  # Start emp_no at 1000000
                min(batch_size, total_employees - batch_start),
                fake
            )
            
            # Insert batch data
            cursor.executemany("""
                INSERT INTO employees 
                (emp_no, birth_date, first_name, last_name, gender, hire_date)
                VALUES (%s, %s, %s, %s, %s, %s)
            """, employees)
            
            cursor.executemany("""
                INSERT INTO salaries 
                (emp_no, salary, from_date, to_date)
                VALUES (%s, %s, %s, %s)
            """, salaries)
            
            cursor.executemany("""
                INSERT INTO dept_emp 
                (emp_no, dept_no, from_date, to_date)
                VALUES (%s, %s, %s, %s)
            """, dept_assignments)
            
            conn.commit()
            
            elapsed = time.time() - start_time
            logger.info(
                f"Batch {batch_start//batch_size + 1}/{(total_employees+batch_size-1)//batch_size} "
                f"completed in {elapsed:.2f}s"
            )
            
    except Exception as e:
        logger.error(f"Error loading data: {str(e)}")
        conn.rollback()
        raise
    finally:
        if 'conn' in locals():
            cursor.close()
            conn.close()

if __name__ == "__main__":
    main()
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\db-setup\scripts\run_load_data.sh ######
#!/bin/bash
set -e

echo "Waiting for MySQL to be ready..."
while ! mysqladmin ping -h"localhost" --silent; do
    sleep 2
done

echo "Starting data load process..."
python3 /docker-entrypoint-initdb.d/load_data.py

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\deploy\azure\deploy.sh ######
#!/bin/bash
set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "${SCRIPT_DIR}/../common/validate.sh"

# Validate environment
validate_environment "azure"

# Load environment variables
source "${SCRIPT_DIR}/../../config/azure.env"

echo "Starting Azure VM native deployment..."

# Install MySQL if not present
if ! command -v mysql &> /dev/null; then
    echo "Installing MySQL..."
    sudo microdnf update
    sudo microdnf install -y mysql-server
fi

# Configure MySQL
echo "Configuring MySQL..."
sudo cp "${SCRIPT_DIR}/../../config/mysql/azure.cnf" /etc/mysql/conf.d/performance.cnf
sudo systemctl restart mysql

# Initialize database
echo "Initializing database..."
mysql -u root -p"${MYSQL_ROOT_PASSWORD}" < "${SCRIPT_DIR}/../../db-setup/migrations/V1__init_monitoring.sql"
mysql -u root -p"${MYSQL_ROOT_PASSWORD}" < "${SCRIPT_DIR}/../../db-setup/migrations/V2__create_base_schema.sql"
mysql -u root -p"${MYSQL_ROOT_PASSWORD}" < "${SCRIPT_DIR}/../../db-setup/migrations/V3__add_indexes.sql"
mysql -u root -p"${MYSQL_ROOT_PASSWORD}" < "${SCRIPT_DIR}/../../db-setup/migrations/V4__create_views.sql"

# Initialize data
echo "Initializing data..."
source "${SCRIPT_DIR}/../common/init-data.sh"

# Setup monitoring
echo "Setting up monitoring..."
source "${SCRIPT_DIR}/setup-monitoring.sh"

echo "Deployment complete!"

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\deploy\common\init-data.sh ######
#!/bin/bash

# Sample script to initialize data
# Add your data initialization commands here

echo "Initializing sample data..."
# Example command to insert data
# mysql -u root -p"${MYSQL_ROOT_PASSWORD}" -e "INSERT INTO employees (name) VALUES ('John Doe');"

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\deploy\common\validate.sh ######
#!/bin/bash

validate_environment() {
    local deploy_type=$1
    
    # Check for required tools
    case $deploy_type in
        "docker")
            command -v docker >/dev/null 2>&1 || { echo "Docker is required but not installed."; exit 1; }
            command -v docker-compose >/dev/null 2>&1 || { echo "Docker Compose is required but not installed."; exit 1; }
            ;; 
        "azure")
            command -v mysql >/dev/null 2>&1 || { echo "MySQL client is required but not installed."; exit 1; }
            ;;
    esac
    
    # Check for required files
    local required_files=(
        "../../config/mysql/${deploy_type}.cnf"
        "../../config/${deploy_type}.env"
        "../../db-setup/migrations/V1__init_monitoring.sql"
    )
    
    for file in "${required_files[@]}"; do
        if [[ ! -f "${SCRIPT_DIR}/${file}" ]]; then
            echo "Required file not found: ${file}"
            exit 1
        fi
    done
}

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\deploy\docker\deploy.sh ######
#!/bin/bash
set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "${SCRIPT_DIR}/../common/validate.sh"

# Validate environment
validate_environment "docker"

# Load environment variables
source "${SCRIPT_DIR}/../../config/docker.env"

echo "Starting Docker deployment..."

# Ensure Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "Docker is not running. Starting Docker..."
    sudo systemctl start docker
fi

# Pull required images
echo "Pulling required Docker images..."
docker-compose pull

# Start services
echo "Starting services..."
docker-compose up -d mysql

# Wait for MySQL to be ready
echo "Waiting for MySQL to be ready..."
until docker-compose exec -T mysql mysqladmin ping -h"localhost" -u"$MYSQL_USER" -p"$MYSQL_PASSWORD" --silent; do
    echo "MySQL is unavailable - sleeping"
    sleep 5
done

# Initialize data
echo "Initializing data..."
source "${SCRIPT_DIR}/../common/init-data.sh"

# Start remaining services
echo "Starting API and load generator..."
docker-compose up -d api load-generator

echo "Deployment complete!"

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\infrastructure\newrelic\custom-monitors.yml ######
custom_monitors:
  slow_queries:
    threshold: 1
    period: 60
    metric_name: SlowQueryCount
    query: |
      SELECT COUNT(*) as count
      FROM performance_schema.events_statements_history
      WHERE TIMER_WAIT > 1000000000000

  lock_contentions:
    threshold: 5
    period: 60
    metric_name: LockContentionCount
    query: |
      SELECT COUNT(*) as count
      FROM performance_schema.events_waits_current
      WHERE EVENT_NAME LIKE 'wait/lock%'

  memory_pressure:
    threshold: 90
    period: 60
    metric_name: MemoryPressure
    query: |
      SELECT SUBSTRING_INDEX(event_name,'/',2) as event_type,
             SUM(current_alloc) as current_bytes
      FROM performance_schema.memory_summary_global_by_event_name
      GROUP BY SUBSTRING_INDEX(event_name,'/',2)

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\infrastructure\newrelic\Dockerfile ######
# infrastructure/newrelic/Dockerfile

# Build stage for nri-mysql using a specific commit and modifying code
FROM golang:1.23-alpine AS builder

# Install git and build essentials
RUN apk add --no-cache git make

# Hardcode the integration ref
ARG MYSQL_INTEGRATION_REF=703b1f6

WORKDIR /go/src/github.com/newrelic/nri-mysql

# Clone the repository at the specified commit
RUN git clone https://github.com/spathlavath/nri-mysql.git . && \
    git checkout ${MYSQL_INTEGRATION_REF}

# Use sed to remove the conditional block and always call PopulateQueryPerformanceMetrics
RUN sed -i '/if args.EnableQueryPerformanceMonitoring {/,/}/c\query_performance_details.PopulateQueryPerformanceMetrics(args, e, i)' src/mysql.go

# Compile the modified binary
RUN make compile

# Final stage
FROM newrelic/infrastructure-bundle:latest

# Backup original binary if it exists
RUN if [ -f /var/db/newrelic-infra/newrelic-integrations/bin/nri-mysql ]; then \
    mv /var/db/newrelic-infra/newrelic-integrations/bin/nri-mysql /var/db/newrelic-infra/newrelic-integrations/bin/nri-mysql.bak; \
    fi

# Copy the compiled binary from builder
COPY --from=builder /go/src/github.com/newrelic/nri-mysql/bin/nri-mysql /var/db/newrelic-infra/newrelic-integrations/bin/

# Set correct permissions
RUN chmod 755 /var/db/newrelic-infra/newrelic-integrations/bin/nri-mysql

# Verify binary works
RUN /var/db/newrelic-infra/newrelic-integrations/bin/nri-mysql -show_version

ENTRYPOINT ["/usr/bin/newrelic-infra"]

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\infrastructure\newrelic\newrelic-infra.yml ######
license_key: ${NEW_RELIC_LICENSE_KEY}
display_name: MySQL-Performance-Demo
custom_attributes:
  environment: performance_testing
  service: mysql_performance

enable_process_metrics: true
strip_command_line: false

log:
  level: info
  file: /var/log/newrelic-infra/newrelic-infra.log

# Integration configurations
integrations_config:
  protocol_version: 4
  interval: 15s

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\infrastructure\newrelic\integrations.d\mysql-config.yml ######
integrations:
  - name: nri-mysql
    interval: 15s
    command: all_data
    env:
      HOSTNAME: ${MYSQL_HOST}
      PORT: ${MYSQL_PORT}
      USERNAME: ${MYSQL_MONITOR_USER}
      PASSWORD: ${MYSQL_MONITOR_PASSWORD}
      REMOTE_MONITORING: true
      EXTENDED_METRICS: true
      EXTENDED_INNODB_METRICS: true
      EXTENDED_PERFORMANCE_METRICS: true
      TABLES_METRICS: true
    labels:
      environment: performance_testing
      role: primary
    inventory_source: config/mysql
    config:
      # Collect specific query metrics
      slow_query_metrics: true
      query_response_time_stats: true
      
      # InnoDB metrics collection
      innodb_metrics:
        buffer_pool_metrics: true
        lock_metrics: true
        page_metrics: true
        
      # Table metrics collection
      table_metrics:
        size_metrics: true
        index_metrics: true
        
      # Specific table monitoring
      tables:
        - employees
        - departments
        - dept_emp
        - salaries

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\monitoring\newrelic\config.yml ######
integrations:
  - name: nri-mysql
    interval: 15s
    command: all_data
    env:
      HOSTNAME: ${MYSQL_HOST}
      PORT: ${MYSQL_PORT}
      USERNAME: ${MYSQL_MONITOR_USER}
      PASSWORD: ${MYSQL_MONITOR_PASSWORD}
      REMOTE_MONITORING: true
      EXTENDED_METRICS: true
      EXTENDED_INNODB_METRICS: true
      EXTENDED_PERFORMANCE_METRICS: true
      TABLES_METRICS: true
    
    config:
      slow_query_metrics: true
      query_response_time_stats: true
      
      innodb_metrics:
        buffer_pool_metrics: true
        lock_metrics: true
        page_metrics: true
        
      table_metrics:
        size_metrics: true
        index_metrics: true
        
      tables:
        - employees
        - departments
        - dept_emp
        - salaries

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\scripts\deploy-local.sh ######
#!/bin/bash
set -e
echo "[INFO] Deploying local environment..."
docker-compose down --remove-orphans
docker-compose up -d --build
echo "[INFO] Environment deployed!"
docker-compose ps

# Call verify_environment.sh
bash verify_environment.sh

# Call verify_environment.sh
bash verify_environment.sh

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\Dockerfile ######
FROM node:20-alpine

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm install --omit=dev && \
    npm cache clean --force

# Copy application files
COPY . .

EXPOSE 3000

CMD ["node", "server.js"]
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\newrelic.cjs ######
'use strict';

exports.config = {
  app_name: [process.env.NEW_RELIC_APP_NAME],
  license_key: process.env.NEW_RELIC_LICENSE_KEY,
  logging: {
    level: 'info',
    enabled: true
  },
  allow_all_headers: true,
  attributes: {
    exclude: [
      'request.headers.cookie',
      'request.headers.authorization',
      'request.headers.proxyAuthorization',
      'request.headers.setCookie*',
      'request.headers.x*',
      'response.headers.cookie',
      'response.headers.authorization',
      'response.headers.proxyAuthorization',
      'response.headers.setCookie*',
      'response.headers.x*'
    ]
  },
  slow_sql: {
    enabled: true
  }
};

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\package.json ######
{
  "name": "api-service",
  "version": "1.0.0",
  "type": "module",
  "scripts": {
    "start": "node server.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "mysql2": "^3.6.1",
    "newrelic": "^12.8.1",
    "express-async-handler": "^1.2.0"
  }
}
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\server.js ######
import 'newrelic';
import express from 'express';
import { createPool } from 'mysql2/promise';
import asyncHandler from 'express-async-handler';

const app = express();

const pool = createPool({
    host: process.env.MYSQL_HOST,
    port: parseInt(process.env.MYSQL_PORT || '3306', 10),
    user: process.env.MYSQL_USER,
    password: process.env.MYSQL_PASSWORD,
    database: process.env.MYSQL_DATABASE,
    waitForConnections: true,
    connectionLimit: 25,
    queueLimit: 0,
    enableKeepAlive: false
});

// Complex analytical query
app.get('/employee-analysis', asyncHandler(async (req, res) => {
    const query = `
        WITH RECURSIVE employee_hierarchy AS (
            SELECT 
                e.emp_no,
                e.first_name,
                e.last_name,
                d.dept_name,
                s.salary,
                1 as level,
                e.salary_tier,
                CAST(CONCAT(e.emp_no) AS CHAR(1000)) as path
            FROM employees e
            JOIN dept_emp de ON e.emp_no = de.emp_no
            JOIN departments d ON de.dept_no = d.dept_no
            JOIN salaries s ON e.emp_no = s.emp_no
            WHERE s.to_date = '9999-01-01'
            AND (e.last_name LIKE 'A%' OR e.last_name LIKE 'B%')
            
            UNION ALL
            
            SELECT 
                e2.emp_no,
                e2.first_name,
                e2.last_name,
                d2.dept_name,
                s2.salary,
                h.level + 1,
                e2.salary_tier,
                CONCAT(h.path, ',', e2.emp_no)
            FROM employee_hierarchy h
            JOIN dept_emp de2 ON h.emp_no = de2.emp_no
            JOIN departments d2 ON de2.dept_no = d2.dept_no
            JOIN employees e2 ON de2.emp_no = e2.emp_no
            JOIN salaries s2 ON e2.emp_no = s2.emp_no
            WHERE s2.to_date = '9999-01-01'
            AND h.level < 3
        )
        SELECT 
            dept_name,
            COUNT(DISTINCT emp_no) as emp_count,
            AVG(salary) as avg_salary,
            MAX(level) as hierarchy_depth,
            GROUP_CONCAT(DISTINCT CONCAT(first_name, ' ', last_name) 
                ORDER BY salary DESC SEPARATOR '; ' LIMIT 10) as top_earners,
            SUM(CASE WHEN salary_tier = 1 THEN 1 ELSE 0 END) as tier1_count,
            SUM(CASE WHEN salary_tier = 4 THEN 1 ELSE 0 END) as tier4_count,
            path
        FROM employee_hierarchy
        GROUP BY dept_name, path
        HAVING avg_salary > (
            SELECT AVG(salary) * 0.8 FROM salaries WHERE to_date = '9999-01-01'
        )
        ORDER BY avg_salary DESC`;

    const [results] = await pool.query(query);
    res.json(results);
}));

// Memory-intensive operation
app.get('/salary-distribution', asyncHandler(async (req, res) => {
    const detailed = req.query.detailed === 'true';
    const conn = await pool.getConnection();
    
    try {
        await conn.query('DROP TEMPORARY TABLE IF EXISTS salary_stats');
        await conn.query(`
            CREATE TEMPORARY TABLE salary_stats AS
            SELECT 
                d.dept_name,
                e.gender,
                e.birth_month,
                e.hire_year,
                e.salary_tier,
                s.salary,
                NTILE(100) OVER (PARTITION BY d.dept_name ORDER BY s.salary) as percentile,
                COUNT(*) OVER (PARTITION BY d.dept_name) as dept_count,
                AVG(s.salary) OVER (
                    PARTITION BY d.dept_name, e.gender
                    ORDER BY s.from_date
                    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
                ) as cumulative_avg_by_gender,
                DENSE_RANK() OVER (ORDER BY s.salary DESC) as salary_rank
            FROM employees e
            JOIN dept_emp de ON e.emp_no = de.emp_no
            JOIN departments d ON de.dept_no = d.dept_no
            JOIN salaries s ON e.emp_no = s.emp_no
            WHERE s.to_date = '9999-01-01'`);

        const query = detailed ? `
            SELECT 
                dept_name,
                gender,
                birth_month,
                hire_year,
                salary_tier,
                AVG(salary) as avg_salary,
                MIN(salary) as min_salary,
                MAX(salary) as max_salary,
                COUNT(*) as employee_count,
                AVG(cumulative_avg_by_gender) as avg_cumulative_by_gender,
                GROUP_CONCAT(DISTINCT percentile ORDER BY percentile) as percentile_distribution
            FROM salary_stats
            GROUP BY 
                dept_name, 
                gender, 
                birth_month,
                hire_year,
                salary_tier
            WITH ROLLUP` :
            `SELECT 
                dept_name,
                gender,
                AVG(salary) as avg_salary,
                COUNT(*) as employee_count
            FROM salary_stats
            GROUP BY dept_name, gender`;

        const [results] = await conn.query(query);
        res.json(results);
    } finally {
        conn.release();
    }
}));

// Lock contention generator
app.get('/update-salaries', asyncHandler(async (req, res) => {
    const connections = await Promise.all(Array(5).fill().map(() => pool.getConnection()));
    
    try {
        await Promise.all(connections.map(async (conn, index) => {
            await conn.beginTransaction();
            
            const targetTier = (index % 4) + 1;
            const query = `
                UPDATE salaries s
                JOIN employees e ON s.emp_no = e.emp_no
                JOIN dept_emp de ON e.emp_no = de.emp_no
                JOIN departments d ON de.dept_no = d.dept_no
                SET s.salary = s.salary * ?
                WHERE e.salary_tier = ?
                AND s.to_date = '9999-01-01'
                AND de.to_date = '9999-01-01'`;
            
            const multiplier = 1 + (Math.random() * 0.2 - 0.1); // Â±10%
            
            await conn.query(query, [multiplier, targetTier]);
            await new Promise(resolve => setTimeout(resolve, 500));
            await conn.commit();
        }));
        
        res.json({ status: 'success' });
    } catch (error) {
        await Promise.all(connections.map(conn => conn.rollback()));
        throw error;
    } finally {
        connections.forEach(conn => conn.release());
    }
}));

// Health check
app.get('/health', asyncHandler(async (req, res) => {
    await pool.query('SELECT 1');
    res.json({ status: 'healthy' });
}));

app.use((err, req, res, next) => {
    console.error(err);
    res.status(500).json({ error: err.message });
});

const port = process.env.API_PORT || 3000;
app.listen(port, () => {
    console.log(`Server running on port ${port}`);
});

process.on('SIGTERM', async () => {
    await pool.end();
    process.exit(0);
});
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\api\database\schema.sql ######
-- Create employees table
CREATE TABLE employees (
    emp_no INT PRIMARY KEY,
    first_name VARCHAR(255) NOT NULL,
    last_name VARCHAR(255) NOT NULL,
    gender ENUM('M', 'F') NOT NULL,
    birth_date DATE NOT NULL,
    hire_date DATE NOT NULL,
    salary_tier INT NOT NULL
);

-- Create departments table
CREATE TABLE departments (
    dept_no CHAR(4) PRIMARY KEY,
    dept_name VARCHAR(255) NOT NULL
);

-- Create dept_emp table
CREATE TABLE dept_emp (
    emp_no INT NOT NULL,
    dept_no CHAR(4) NOT NULL,
    from_date DATE NOT NULL,
    to_date DATE NOT NULL,
    PRIMARY KEY (emp_no, dept_no),
    FOREIGN KEY (emp_no) REFERENCES employees(emp_no),
    FOREIGN KEY (dept_no) REFERENCES departments(dept_no)
);

-- Create salaries table
CREATE TABLE salaries (
    emp_no INT NOT NULL,
    salary DECIMAL(10, 2) NOT NULL,
    from_date DATE NOT NULL,
    to_date DATE NOT NULL,
    PRIMARY KEY (emp_no, from_date),
    FOREIGN KEY (emp_no) REFERENCES employees(emp_no)
);

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\load-generator\Dockerfile ######
FROM grafana/k6:latest

# Copy test scripts
COPY scripts/ /scripts/

WORKDIR /scripts

# Default command to run k6
CMD ["run", "load-test.js"]
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\mysql\services\load-generator\scripts\load-test.js ######
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
    scenarios: {
        // Heavy analytical queries
        analytical_load: {
            executor: 'constant-vus',
            vus: 10,
            duration: '30m',
            exec: 'runAnalyticalQueries'
        },
        
        // Lock contention generator
        lock_contention: {
            executor: 'ramping-arrival-rate',
            startRate: 5,
            timeUnit: '1s',
            preAllocatedVUs: 20,
            stages: [
                { duration: '5m', target: 10 },
                { duration: '10m', target: 20 },
                { duration: '5m', target: 5 }
            ],
            exec: 'generateLockContention'
        },
        
        // Memory pressure generator
        memory_pressure: {
            executor: 'per-vu-iterations',
            vus: 15,
            iterations: 100,
            maxDuration: '30m',
            exec: 'generateMemoryPressure'
        },
        
        // Mixed workload
        mixed_load: {
            executor: 'ramping-vus',
            startVUs: 0,
            stages: [
                { duration: '5m', target: 20 },
                { duration: '10m', target: 40 },
                { duration: '5m', target: 10 }
            ],
            exec: 'runMixedWorkload'
        }
    },
    thresholds: {
        http_req_failed: ['rate<0.05'],
        http_req_duration: ['p(95)<5000']
    }
};

const BASE_URL = __ENV.API_URL || 'http://localhost:3000';

function makeRequest(url, name) {
    const response = http.get(`${BASE_URL}${url}`);
    check(response, {
        [`${name} status was 200`]: (r) => r.status === 200,
        [`${name} duration < 5s`]: (r) => r.timings.duration < 5000
    });
    return response;
}

// Heavy analytical queries
export function runAnalyticalQueries() {
    makeRequest('/employee-analysis?detailed=true', 'Employee Analysis');
    sleep(Math.random() * 2 + 1);
    
    makeRequest('/salary-distribution?detailed=true', 'Salary Distribution');
    sleep(Math.random() * 3 + 2);
}

// Lock contention generator
export function generateLockContention() {
    const requests = [];
    for (let i = 0; i < 3; i++) {
        requests.push(['GET', `${BASE_URL}/update-salaries`]);
    }
    
    const responses = http.batch(requests);
    responses.forEach((response, index) => {
        check(response, {
            [`Lock contention ${index + 1} status was 200`]: (r) => r.status === 200
        });
    });
    
    sleep(Math.random() * 1 + 0.5);
}

// Memory pressure generator
export function generateMemoryPressure() {
    const requests = [];
    for (let i = 0; i < 5; i++) {
        requests.push(['GET', `${BASE_URL}/salary-distribution?detailed=true`]);
    }
    
    const responses = http.batch(requests);
    responses.forEach((response, index) => {
        check(response, {
            [`Memory pressure ${index + 1} status was 200`]: (r) => r.status === 200
        });
    });
    
    sleep(Math.random() * 2 + 1);
}

// Mixed workload
export function runMixedWorkload() {
    const endpoints = [
        { url: '/employee-analysis', weight: 2 },
        { url: '/salary-distribution', weight: 3 },
        { url: '/update-salaries', weight: 1 }
    ];
    
    let totalWeight = endpoints.reduce((sum, endpoint) => sum + endpoint.weight, 0);
    let random = Math.random() * totalWeight;
    
    for (const endpoint of endpoints) {
        if (random < endpoint.weight) {
            makeRequest(endpoint.url, 'Mixed Workload');
            break;
        }
        random -= endpoint.weight;
    }
    
    sleep(Math.random() * 1 + 0.5);
}

e
