###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\.env.example ######
# Core Configuration
NEW_RELIC_LICENSE_KEY=your_license_key_here
NEW_RELIC_APP_NAME=postgres-perf-demo

# Database Settings
POSTGRES_PASSWORD=demo123
POSTGRES_DB=employees
POSTGRES_USER=postgres
POSTGRES_MONITOR_USER=newrelic
POSTGRES_MONITOR_PASSWORD=newrelic123

# Infrastructure Settings
NRIA_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
NRIA_DISPLAY_NAME=postgres-perf-infra
POSTGRES_INTEGRATION_BRANCH=master

# Performance Test Settings
API_PORT=3000
K6_VUS=10
K6_DURATION=5m
NODE_ENV=production

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\.gitignore ######
.env
*.log
node_modules/
db-setup/logs/*
!db-setup/logs/.gitkeep

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\combined_content.txt ######
###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\.env.example ######
# Core Configuration
NEW_RELIC_LICENSE_KEY=your_license_key_here
NEW_RELIC_APP_NAME=postgres-perf-demo

# Database Settings
POSTGRES_PASSWORD=demo123
POSTGRES_DB=employees
POSTGRES_USER=postgres
POSTGRES_MONITOR_USER=newrelic
POSTGRES_MONITOR_PASSWORD=newrelic123

# Infrastructure Settings
NRIA_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
NRIA_DISPLAY_NAME=postgres-perf-infra
POSTGRES_INTEGRATION_BRANCH=master

# Performance Test Settings
API_PORT=3000
K6_VUS=10
K6_DURATION=5m
NODE_ENV=production

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\.gitignore ######
.env
*.log
node_modules/
db-setup/logs/*
!db-setup/logs/.gitkeep


###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\docker-compose.yml ######
version: '3.8'

services:
  postgres:
    build:
      context: ./db-setup
    container_name: postgres-perf
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_MONITOR_USER=${POSTGRES_MONITOR_USER}
      - POSTGRES_MONITOR_PASSWORD=${POSTGRES_MONITOR_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./db-setup/config:/etc/postgresql:ro
      - ./db-setup/logs:/var/log/postgresql:rw
    command: ["postgres", "-c", "config_file=/etc/postgresql/postgresql.conf"]
    healthcheck:
      test: pg_isready -U postgres
      interval: 10s
      timeout: 5s
      retries: 3
    ports:
      - "5432:5432"

  newrelic-infra-bundle:
    build:
      context: ./infrastructure/newrelic
      dockerfile: Dockerfile
      args:
        - POSTGRES_INTEGRATION_BRANCH=${POSTGRES_INTEGRATION_BRANCH:-master}
    container_name: infrastructure-bundle
    cap_add:
      - SYS_PTRACE
    privileged: true
    pid: host
    network_mode: bridge
    volumes:
      - /:/host:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - ./infrastructure/newrelic/integrations.d/:/etc/newrelic-infra/integrations.d/:ro
    environment:
      - NRIA_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
      - NRIA_DISPLAY_NAME=postgres-performance-testing
      - NRIA_VERBOSE=1
      - NR_POSTGRES_USERNAME=${POSTGRES_MONITOR_USER}
      - NR_POSTGRES_PASSWORD=${POSTGRES_MONITOR_PASSWORD}
      - NR_POSTGRES_HOSTNAME=postgres-perf
      - NR_POSTGRES_PORT=5432
      - NR_POSTGRES_DATABASE=${POSTGRES_DB}
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped

  api:
    build: ./api
    container_name: postgres-api
    environment:
      - DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
      - NEW_RELIC_APP_NAME=${NEW_RELIC_APP_NAME}-api
      - PORT=${API_PORT:-3000}
      - NODE_ENV=${NODE_ENV:-production}
    ports:
      - "${API_PORT:-3000}:3000"
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      postgres:
        condition: service_healthy

  k6:
    build: ./k6
    container_name: postgres-load
    environment:
      - API_URL=http://api:${API_PORT:-3000}
      - NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
      - NEW_RELIC_APP_NAME=${NEW_RELIC_APP_NAME}-load
      - K6_VUS=${K6_VUS:-10}
      - K6_DURATION=${K6_DURATION:-5m}
    depends_on:
      api:
        condition: service_healthy

networks:
  default:
    name: postgres-perf-network

volumes:
  postgres_data:

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\api\Dockerfile ######
FROM node:20-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY . .

EXPOSE 3000

CMD ["node", "server.js"]

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\api\newrelic.js ######
'use strict';

exports.config = {
  app_name: [process.env.NEW_RELIC_APP_NAME || 'postgres-perf-demo-api'],
  license_key: process.env.NEW_RELIC_LICENSE_KEY,
  logging: {
    level: 'info',
    enabled: true
  },
  allow_all_headers: true,
  attributes: {
    exclude: [
      'request.headers.cookie',
      'request.headers.authorization',
      'request.headers.proxyAuthorization',
      'request.headers.setCookie*',
      'request.headers.x*',
      'response.headers.cookie',
      'response.headers.authorization',
      'response.headers.proxyAuthorization',
      'response.headers.setCookie*',
      'response.headers.x*'
    ]
  },
  distributed_tracing: {
    enabled: true
  },
  transaction_tracer: {
    record_sql: 'raw',
    explain_threshold: 500,
    enabled: true
  },
  slow_sql: {
    enabled: true
  }
};

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\api\package.json ######
{
  "name": "postgres-perf-api",
  "version": "1.0.0",
  "private": true,
  "scripts": {
    "start": "node server.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "pg": "^8.11.3",
    "newrelic": "^11.7.0"
  }
}

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\api\server.js ######
require('newrelic');
const express = require('express');
const { Pool } = require('pg');

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

const app = express();

// Middleware to track all requests
app.use((req, res, next) => {
  const startTime = Date.now();
  res.on('finish', () => {
    const duration = Date.now() - startTime;
    newrelic.recordMetric('Custom/API/RequestDuration', duration);
  });
  next();
});

// Health check
app.get('/health', async (req, res) => {
  try {
    await pool.query('SELECT 1');
    res.json({ status: 'healthy' });
  } catch (err) {
    res.status(500).json({ status: 'unhealthy', error: err.message });
  }
});

// Complex query endpoint
app.get('/query/complex', async (req, res) => {
  const client = await pool.connect();
  try {
    const searchPattern = `${['A','B','C'][Math.floor(Math.random() * 3)]}%`;
    const salaryThreshold = 50000 + Math.floor(Math.random() * 50000);

    const { rows } = await client.query(`
      WITH RankedEmployees AS (
        SELECT 
          e.emp_no,
          e.name_upper,
          d.dept_name,
          s.salary,
          ROW_NUMBER() OVER (PARTITION BY d.dept_no ORDER BY s.salary DESC) as salary_rank
        FROM employees e
        JOIN dept_emp de ON e.emp_no = de.emp_no
        JOIN departments d ON de.dept_no = d.dept_no
        JOIN salaries s ON e.emp_no = s.emp_no
        WHERE 
          e.name_upper LIKE $1
          AND s.salary > $2
          AND de.to_date = '9999-12-31'
      )
      SELECT * FROM RankedEmployees WHERE salary_rank <= 10
    `, [searchPattern, salaryThreshold]);

    res.json({ rows });
  } catch (err) {
    newrelic.noticeError(err);
    res.status(500).json({ error: err.message });
  } finally {
    client.release();
  }
});

// Lock generation endpoint
app.get('/query/lock', async (req, res) => {
  const client = await pool.connect();
  try {
    await client.query('BEGIN');
    
    const { rows: [employee] } = await client.query(`
      SELECT emp_no FROM employees 
      WHERE emp_no IN (
        SELECT emp_no FROM salaries 
        WHERE salary > 100000
      )
      ORDER BY RANDOM() 
      LIMIT 1
    `);

    if (!employee) {
      await client.query('ROLLBACK');
      throw new Error('No suitable employee found');
    }

    await client.query('SELECT pg_sleep(1)');
    await client.query(`
      UPDATE employees 
      SET last_name = last_name || '_updated' 
      WHERE emp_no = $1
    `, [employee.emp_no]);
    
    await client.query('COMMIT');
    res.json({ status: 'success', emp_no: employee.emp_no });
  } catch (err) {
    await client.query('ROLLBACK');
    newrelic.noticeError(err);
    res.status(500).json({ error: err.message });
  } finally {
    client.release();
  }
});

// Stats refresh endpoint
app.get('/query/stats', async (req, res) => {
  const client = await pool.connect();
  try {
    const startTime = Date.now();
    await client.query('REFRESH MATERIALIZED VIEW dept_stats');
    const duration = Date.now() - startTime;
    
    res.json({ 
      status: 'success', 
      duration,
      refreshed: new Date().toISOString() 
    });
  } catch (err) {
    newrelic.noticeError(err);
    res.status(500).json({ error: err.message });
  } finally {
    client.release();
  }
});

const port = process.env.PORT || 3000;
app.listen(port, () => console.log(`API running on port ${port}`));

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\.env.example ######
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password
POSTGRES_DB=postgres
POSTGRES_MONITOR_USER=monitor
POSTGRES_MONITOR_PASSWORD=monitorpass
BATCH_SIZE=1000
TOTAL_EMPLOYEES=100000

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\Dockerfile ######
FROM postgres:15

# Install prerequisites
RUN apt-get update && \
    apt-get install -y python3 python3-pip tzdata && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Install New Relic Python agent
RUN pip3 install --no-cache-dir newrelic

ENV TZ=UTC

# Python requirements
COPY requirements.txt /tmp/requirements.txt
RUN pip3 install --no-cache-dir -r /tmp/requirements.txt

# Copy PostgreSQL config files
# We'll store main configs in /etc/postgresql/ and a custom conf.d/ directory for included configs
RUN mkdir -p /etc/postgresql/conf.d
COPY configs/pg_hba.conf /etc/postgresql/pg_hba.conf
COPY configs/postgresql.conf /etc/postgresql/postgresql.conf
COPY configs/conf.d/custom.conf /etc/postgresql/conf.d/custom.conf

# Copy migrations and scripts
COPY migrations/*.sql /docker-entrypoint-initdb.d/
COPY scripts/*.py /docker-entrypoint-initdb.d/
COPY scripts/*.sh /docker-entrypoint-initdb.d/

# Set correct permissions
RUN chown -R postgres:postgres /etc/postgresql && \
    chmod 0444 /etc/postgresql/pg_hba.conf && \
    chmod 0444 /etc/postgresql/postgresql.conf && \
    chmod 0444 /etc/postgresql/conf.d/custom.conf && \
    chown -R postgres:postgres /docker-entrypoint-initdb.d && \
    chmod 0444 /docker-entrypoint-initdb.d/*.sql && \
    chmod 0555 /docker-entrypoint-initdb.d/*.py && \
    chmod 0555 /docker-entrypoint-initdb.d/*.sh

EXPOSE 5432

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\requirements.txt ######
psycopg2-binary==2.9.6
Faker==19.13.0

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\config\pg_hba.conf ######
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all            postgres                                trust
host    all            all             127.0.0.1/32           scram-sha-256
host    all            all             172.16.0.0/12          scram-sha-256
host    all            ${POSTGRES_MONITOR_USER}    0.0.0.0/0  scram-sha-256
host    all            ${POSTGRES_USER}           0.0.0.0/0   scram-sha-256

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\config\postgresql.conf ######
# Memory Configuration
shared_buffers = '1GB'
work_mem = '64MB'
maintenance_work_mem = '256MB'
effective_cache_size = '3GB'

# Performance Monitoring
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.max = 10000
pg_stat_statements.track = all
pg_stat_statements.track_utility = on

# Query Planning
random_page_cost = 1.1
effective_io_concurrency = 200
default_statistics_target = 1000

# Logging
log_min_duration_statement = 1000
log_lock_waits = on
log_temp_files = 0
log_checkpoints = on
log_connections = on
log_disconnections = on
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '

# Autovacuum
autovacuum = on
autovacuum_vacuum_scale_factor = 0.1
autovacuum_analyze_scale_factor = 0.05

# WAL
wal_level = logical
max_wal_size = '1GB'
min_wal_size = '80MB'

# Connections
max_connections = 200

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\configs\pg_hba.conf ######
# Host-based authentication configuration file (pg_hba.conf)
# Adjust the below lines as per your user and network requirements.

# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all            postgres                                trust
host    all            all             127.0.0.1/32           scram-sha-256
host    all            all             172.16.0.0/12          scram-sha-256
host    all            ${POSTGRES_MONITOR_USER}    0.0.0.0/0  scram-sha-256
host    all            ${POSTGRES_USER}           0.0.0.0/0   scram-sha-256

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\configs\postgresql.conf ######
# Main PostgreSQL configuration file

# Include custom configs
include_dir = '/etc/postgresql/conf.d'
hba_file = '/etc/postgresql/pg_hba.conf'

# Logging
log_min_duration_statement = 1000
log_lock_waits = on
log_temp_files = 0
log_checkpoints = on
log_connections = on
log_disconnections = on
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '

# WAL
wal_level = logical
max_wal_size = '1GB'
min_wal_size = '80MB'

# Connections
max_connections = 200

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\configs\conf.d\custom.conf ######
# Custom configuration parameters included by postgresql.conf

# Memory Configuration
shared_buffers = '256MB'
work_mem = '4MB'
maintenance_work_mem = '64MB'
effective_cache_size = '1GB'

# Performance Monitoring
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.max = 10000
pg_stat_statements.track = all
pg_stat_statements.track_utility = on

# Query Planning
random_page_cost = 4.0
effective_io_concurrency = 5
default_statistics_target = 100

# Autovacuum
autovacuum = on
autovacuum_vacuum_scale_factor = 0.2
autovacuum_analyze_scale_factor = 0.1

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\migrations\V1__init_monitoring.sql ######
-- Enable extensions for monitoring and create a monitoring user
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- Create monitor user
DO $$
BEGIN
    CREATE USER ${POSTGRES_MONITOR_USER} WITH PASSWORD '${POSTGRES_MONITOR_PASSWORD}';
    EXCEPTION WHEN DUPLICATE_OBJECT THEN
    NULL;
END
$$;

GRANT CONNECT ON DATABASE ${POSTGRES_DB} TO ${POSTGRES_MONITOR_USER};
GRANT USAGE ON SCHEMA public TO ${POSTGRES_MONITOR_USER};
GRANT SELECT ON ALL TABLES IN SCHEMA public TO ${POSTGRES_MONITOR_USER};
GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA public TO ${POSTGRES_MONITOR_USER};
GRANT USAGE ON ALL SEQUENCES IN SCHEMA public TO ${POSTGRES_MONITOR_USER};
GRANT pg_monitor TO ${POSTGRES_MONITOR_USER};

-- Create maintenance logging table
CREATE TABLE IF NOT EXISTS maintenance_log (
    id SERIAL PRIMARY KEY,
    operation TEXT NOT NULL,
    start_time TIMESTAMP WITH TIME ZONE NOT NULL,
    end_time TIMESTAMP WITH TIME ZONE,
    success BOOLEAN DEFAULT false,
    duration INTERVAL GENERATED ALWAYS AS (end_time - start_time) STORED
);

-- Function to insert into the maintenance log
CREATE OR REPLACE FUNCTION log_maintenance(
    p_operation TEXT,
    p_start_time TIMESTAMP WITH TIME ZONE,
    p_end_time TIMESTAMP WITH TIME ZONE DEFAULT NULL,
    p_success BOOLEAN DEFAULT FALSE
)
RETURNS VOID AS $$
BEGIN
    INSERT INTO maintenance_log (operation, start_time, end_time, success)
    VALUES (p_operation, p_start_time, p_end_time, p_success);
END;
$$ LANGUAGE plpgsql;

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\migrations\V2__create_base_schema.sql ######
-- Create a base schema and a sample table
CREATE SCHEMA IF NOT EXISTS app;

-- Create base tables
CREATE TABLE app.employees (
    emp_no INT PRIMARY KEY,
    birth_date DATE NOT NULL,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    gender CHAR(1) CHECK (gender IN ('M', 'F')),
    hire_date DATE NOT NULL,
    name_upper TEXT GENERATED ALWAYS AS (UPPER(last_name)) STORED
);

CREATE TABLE app.departments (
    dept_no CHAR(4) PRIMARY KEY,
    dept_name VARCHAR(40) NOT NULL UNIQUE
);

CREATE TABLE app.dept_emp (
    emp_no INT NOT NULL,
    dept_no CHAR(4) NOT NULL,
    from_date DATE NOT NULL,
    to_date DATE NOT NULL,
    PRIMARY KEY (emp_no, dept_no),
    FOREIGN KEY (emp_no) REFERENCES app.employees(emp_no),
    FOREIGN KEY (dept_no) REFERENCES app.departments(dept_no)
);

CREATE TABLE app.salaries (
    emp_no INT NOT NULL,
    salary INT NOT NULL,
    from_date DATE NOT NULL,
    to_date DATE NOT NULL,
    CONSTRAINT pk_salaries PRIMARY KEY (emp_no, from_date),
    CONSTRAINT fk_salaries_employees FOREIGN KEY (emp_no) REFERENCES app.employees(emp_no)
) PARTITION BY RANGE (from_date);

-- Create salary partitions
CREATE TABLE app.salaries_historical PARTITION OF app.salaries
    FOR VALUES FROM (MINVALUE) TO ('2020-01-01');

CREATE TABLE app.salaries_current PARTITION OF app.salaries
    FOR VALUES FROM ('2020-01-01') TO (MAXVALUE);

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\migrations\V3__add_indexes.sql ######
-- Add indexes to improve performance
CREATE INDEX ON app.users (username);

-- Basic indexes
CREATE INDEX idx_employees_name ON employees(last_name, first_name);
CREATE INDEX idx_employees_hire ON employees(hire_date);
CREATE INDEX idx_employees_upper_name ON employees(name_upper);

-- Partial indexes for performance testing
CREATE INDEX idx_high_salary ON salaries(salary)
WHERE salary > 200000;

CREATE INDEX idx_current_emp ON dept_emp(emp_no)
WHERE to_date = '9999-12-31';

-- Competing composite indexes for performance analysis
CREATE INDEX idx_dept_emp_1 ON dept_emp(emp_no, dept_no);
CREATE INDEX idx_dept_emp_2 ON dept_emp(dept_no, emp_no);
CREATE INDEX idx_dept_emp_3 ON dept_emp(from_date, emp_no, dept_no);

-- Add index to materialized view
CREATE UNIQUE INDEX idx_dept_stats ON dept_stats(dept_no);

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\migrations\V4__create_views.sql ######
-- Create a sample view for reporting
CREATE VIEW app.user_summary AS
SELECT
    COUNT(*) AS total_users,
    MAX(created_at) AS latest_user_created
FROM app.users;

-- Employee statistics view
CREATE OR REPLACE VIEW employee_stats AS
SELECT
    date_part('year', birth_date) as birth_year,
    gender,
    COUNT(*) as total_count,
    AVG(date_part('month', birth_date)) as avg_birth_month,
    SUM(CASE WHEN gender = 'M' THEN 1 ELSE 0 END) as male_count
FROM app.employees
GROUP BY date_part('year', birth_date), gender;

-- Salary metrics view (more complex to trigger more work)
CREATE OR REPLACE VIEW salary_metrics AS
SELECT
    d.dept_name,
    date_part('year', s.from_date) as year,
    COUNT(DISTINCT e.emp_no) as employee_count,
    AVG(s.salary) as avg_salary,
    MAX(s.salary) as max_salary,
    MIN(s.salary) as min_salary,
    STDDEV(s.salary) as salary_stddev
FROM
    app.employees e
    JOIN app.dept_emp de ON e.emp_no = de.emp_no
    JOIN app.departments d ON de.dept_no = d.dept_no
    JOIN app.salaries s ON e.emp_no = s.emp_no
WHERE
    s.to_date = '9999-01-01'  -- Current salaries only
GROUP BY
    d.dept_name,
    date_part('year', s.from_date)
ORDER BY d.dept_name, date_part('year', s.from_date);

-- Create materialized view for department statistics
CREATE MATERIALIZED VIEW dept_stats AS
WITH salary_stats AS (
    SELECT
        de.dept_no,
        d.dept_name,
        COUNT(DISTINCT e.emp_no) as emp_count,
        AVG(s.salary) as avg_salary,
        MIN(s.salary) as min_salary,
        MAX(s.salary) as max_salary,
        STDDEV(s.salary) as salary_stddev
    FROM app.departments d
    JOIN app.dept_emp de ON d.dept_no = de.dept_no
    JOIN app.employees e ON de.emp_no = e.emp_no
    JOIN app.salaries s ON e.emp_no = s.emp_no
    WHERE s.to_date = '9999-12-31'
    GROUP BY de.dept_no, d.dept_name
)
SELECT * FROM salary_stats;

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\scripts\00_setup_config.sh ######
#!/usr/bin/env bash
set -e

# This script runs after initdb but before the server starts.
# Copy custom configs into the data directory
cp /etc/postgresql/postgresql.conf $PGDATA/postgresql.conf
cp /etc/postgresql/pg_hba.conf $PGDATA/pg_hba.conf

chown postgres:postgres $PGDATA/postgresql.conf $PGDATA/pg_hba.conf
chmod 0600 $PGDATA/postgresql.conf $PGDATA/pg_hba.conf

echo "Custom configuration applied."

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\scripts\indexes.sql ######
-- Basic indexes
CREATE INDEX idx_employees_name ON employees(last_name, first_name);
CREATE INDEX idx_employees_hire ON employees(hire_date);
CREATE INDEX idx_employees_upper_name ON employees(name_upper);

-- Partial indexes for performance testing
CREATE INDEX idx_high_salary ON salaries(salary) 
WHERE salary > 100000;

CREATE INDEX idx_current_emp ON dept_emp(emp_no) 
WHERE to_date = '9999-12-31';

-- Competing composite indexes for performance analysis
CREATE INDEX idx_dept_emp_1 ON dept_emp(emp_no, dept_no, from_date);
CREATE INDEX idx_dept_emp_2 ON dept_emp(dept_no, emp_no, from_date);
CREATE INDEX idx_dept_emp_3 ON dept_emp(from_date, emp_no, dept_no);

-- Create materialized view for department statistics
CREATE MATERIALIZED VIEW dept_stats AS
WITH salary_stats AS (
    SELECT 
        de.dept_no,
        d.dept_name,
        COUNT(DISTINCT e.emp_no) as emp_count,
        AVG(s.salary) as avg_salary,
        MIN(s.salary) as min_salary,
        MAX(s.salary) as max_salary,
        STDDEV(s.salary) as salary_stddev
    FROM departments d
    JOIN dept_emp de ON d.dept_no = de.dept_no
    JOIN employees e ON de.emp_no = e.emp_no
    JOIN salaries s ON e.emp_no = s.emp_no
    WHERE s.to_date = '9999-12-31'
    GROUP BY de.dept_no, d.dept_name
)
SELECT * FROM salary_stats;

CREATE UNIQUE INDEX idx_dept_stats ON dept_stats(dept_no);

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\scripts\init.sql ######
-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- Create monitor user
DO $$
BEGIN
    CREATE USER ${POSTGRES_MONITOR_USER} WITH PASSWORD '${POSTGRES_MONITOR_PASSWORD}';
    EXCEPTION WHEN DUPLICATE_OBJECT THEN
    NULL;
END
$$;

GRANT pg_monitor TO ${POSTGRES_MONITOR_USER};

-- Create maintenance logging table
CREATE TABLE IF NOT EXISTS maintenance_log (
    id SERIAL PRIMARY KEY,
    operation TEXT NOT NULL,
    start_time TIMESTAMP WITH TIME ZONE NOT NULL,
    end_time TIMESTAMP WITH TIME ZONE,
    success BOOLEAN DEFAULT false,
    duration INTERVAL GENERATED ALWAYS AS (end_time - start_time) STORED
);

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\scripts\load_data.py ######
#!/usr/bin/env python3
import psycopg2
import random
from datetime import date, timedelta
from faker import Faker
import os
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def random_employee(emp_no, fake):
    birth_date = fake.date_between(start_date='-65y', end_date='-20y')
    first_name = fake.first_name()
    last_name = fake.last_name()
    gender = random.choice(['M','F'])
    hire_date = fake.date_between(start_date='-20y', end_date='today')
    return (emp_no, birth_date, first_name, last_name, gender, hire_date)

def random_salary(emp_no, from_date):
    to_date = date(9999,1,1)
    salary = random.randint(30000, 200000)
    return (emp_no, salary, from_date, to_date)

def main():
    try:
        # Get connection details from environment
        db_config = {
            'host': 'localhost',
            'user': os.getenv('POSTGRES_USER', 'postgres'),
            'password': os.getenv('POSTGRES_PASSWORD', 'password'),
            'database': os.getenv('POSTGRES_DB', 'postgres')
        }

        logger.info(f"Connecting to database {db_config['database']} on {db_config['host']}")
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        fake = Faker()

        # Insert departments
        departments = [
            ('d001', 'Marketing'),
            ('d002', 'Finance'),
            ('d003', 'HR'),
            ('d004', 'Engineering'),
            ('d005', 'Sales')
        ]

        logger.info("Inserting departments...")
        cursor.executemany(
            "INSERT INTO departments (dept_no, dept_name) VALUES (%s, %s) ON CONFLICT (dept_no) DO NOTHING",
            departments
        )
        conn.commit()

        # Check if we already have data
        cursor.execute("SELECT COUNT(*) FROM employees")
        count = cursor.fetchone()[0]
        if count > 0:
            logger.info(f"Database already contains {count} employees. Skipping data load.")
            return

        total_employees = int(os.getenv('TOTAL_EMPLOYEES', 100000))  # Increased total employees
        batch_size = int(os.getenv('BATCH_SIZE', 1000))  # Make batch size configurable
        logger.info(f"Starting data load for {total_employees} employees...")

        for batch in range(total_employees // batch_size):
            employees_data = []
            salaries_data = []
            dept_emp_data = []

            for _ in range(batch_size):
                emp_no = random.randint(1000000,9999999)
                emp = random_employee(emp_no, fake)
                employees_data.append(emp)

                # Multiple salary records per employee
                current_date = emp[5]  # hire_date
                for _ in range(random.randint(1, 4)):
                    salaries_data.append(random_salary(emp_no, current_date))
                    current_date += timedelta(days=random.randint(365, 1095))

                # Department assignment
                dept_no = random.choice(departments)[0]
                dept_emp_data.append((emp_no, dept_no, emp[5], date(9999,1,1)))

            try:
                cursor.executemany("""
                    INSERT INTO employees
                    (emp_no, birth_date, first_name, last_name, gender, hire_date)
                    VALUES (%s, %s, %s, %s, %s, %s)
                    ON CONFLICT (emp_no) DO NOTHING
                """, employees_data)

                cursor.executemany("""
                    INSERT INTO salaries
                    (emp_no, salary, from_date, to_date)
                    VALUES (%s, %s, %s, %s)
                    ON CONFLICT (emp_no, from_date) DO NOTHING
                """, salaries_data)

                cursor.executemany("""
                    INSERT INTO dept_emp
                    (emp_no, dept_no, from_date, to_date)
                    VALUES (%s, %s, %s, %s)
                    ON CONFLICT (emp_no, dept_no) DO NOTHING
                """, dept_emp_data)

                conn.commit()
                logger.info(f"Inserted {len(employees_data)} employees, {len(salaries_data)} salaries, and {len(dept_emp_data)} department assignments in this batch.")
                logger.info(f"Completed batch {batch+1}/{total_employees // batch_size}")

            except psycopg2.Error as sql_err:
                logger.error(f"SQL error in batch {batch}: {str(sql_err)}")
                conn.rollback()
                continue
            except Exception as e:
                logger.error(f"Error in batch {batch}: {str(e)}")
                conn.rollback()
                continue

        # Log data loading as a maintenance event
        cursor.execute("SELECT log_maintenance(%s, %s, %s, %s)", ('Data Load', date.today(), date.today(), True))
        conn.commit()

        # Verify data load
        cursor.execute("SELECT COUNT(*) FROM employees")
        final_count = cursor.fetchone()[0]
        logger.info(f"Data load complete. Total employees: {final_count}")

    except Exception as e:
        logger.error(f"Database connection error: {str(e)}")
        raise
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()

if __name__ == "__main__":
    main()

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\scripts\run_load_data.sh ######
#!/bin/bash
set -e

echo "Waiting for PostgreSQL to be ready..."
until pg_isready -h "localhost" -U "${POSTGRES_USER}"
do
    sleep 2
done

echo "Starting data load process..."
python3 /docker-entrypoint-initdb.d/load_data.py

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\db-setup\scripts\schema.sql ######
-- Create base tables
CREATE TABLE employees (
    emp_no INT PRIMARY KEY,
    birth_date DATE NOT NULL,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    gender CHAR(1) CHECK (gender IN ('M', 'F')),
    hire_date DATE NOT NULL,
    name_upper TEXT GENERATED ALWAYS AS (UPPER(last_name)) STORED
);

CREATE TABLE departments (
    dept_no CHAR(4) PRIMARY KEY,
    dept_name VARCHAR(40) NOT NULL UNIQUE
);

CREATE TABLE dept_emp (
    emp_no INT NOT NULL,
    dept_no CHAR(4) NOT NULL,
    from_date DATE NOT NULL,
    to_date DATE NOT NULL,
    PRIMARY KEY (emp_no, dept_no),
    FOREIGN KEY (emp_no) REFERENCES employees(emp_no),
    FOREIGN KEY (dept_no) REFERENCES departments(dept_no)
);

CREATE TABLE salaries (
    emp_no INT NOT NULL,
    salary INT NOT NULL,
    from_date DATE NOT NULL,
    to_date DATE NOT NULL,
    CONSTRAINT pk_salaries PRIMARY KEY (emp_no, from_date),
    CONSTRAINT fk_salaries_employees FOREIGN KEY (emp_no) REFERENCES employees(emp_no)
) PARTITION BY RANGE (from_date);

-- Create salary partitions
CREATE TABLE salaries_historical PARTITION OF salaries
    FOR VALUES FROM (MINVALUE) TO ('2020-01-01');

CREATE TABLE salaries_current PARTITION OF salaries
    FOR VALUES FROM ('2020-01-01') TO (MAXVALUE);

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\infrastructure\newrelic\Dockerfile ######
# Build stage for nri-postgresql
FROM golang:1.21-alpine AS builder

# Install git and build essentials
RUN apk add --no-cache git make

# Set working directory
WORKDIR /go/src/github.com/newrelic

# Clone specific branch of nri-postgresql
ARG POSTGRES_INTEGRATION_BRANCH=master
RUN git clone -b ${POSTGRES_INTEGRATION_BRANCH} https://github.com/newrelic/nri-postgresql.git

# Build the integration
WORKDIR /go/src/github.com/newrelic/nri-postgresql
RUN make compile

# Final stage with New Relic infrastructure bundle
FROM newrelic/infrastructure-bundle:latest

# Copy the compiled binary from builder
COPY --from=builder /go/src/github.com/newrelic/nri-postgresql/bin/nri-postgresql /var/db/newrelic-infra/newrelic-integrations/bin/

# Set correct permissions
RUN chmod 755 /var/db/newrelic-infra/newrelic-integrations/bin/nri-postgresql

# Keep the rest of the infrastructure bundle configuration
ENTRYPOINT ["/usr/bin/newrelic-infra"]

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\infrastructure\newrelic\integrations.d\postgresql-config.yml ######
integrations:
  - name: nri-postgresql
    interval: 15s
    env:
      PG_HOST: postgres-perf
      PG_PORT: 5432
      PG_USERNAME: ${NR_POSTGRES_USERNAME}
      PG_PASSWORD: ${NR_POSTGRES_PASSWORD}
      PG_DATABASE: ${NR_POSTGRES_DATABASE}
      PG_COLLECTION_LIST: ALL
    labels:
      env: performance-testing
      role: primary

    config:
      connection_max_lifetime: 0
      connection_max_idle_connections: 10
      connection_max_open_connections: 50

      collect:
        activity_metrics: true
        database_metrics: true
        table_metrics: true
        index_metrics: true
        statement_metrics: true
        lock_metrics: true
        vacuum_metrics: true

      custom_metrics:
        - query: |
            SELECT queryid, calls, total_time, mean_time, rows
            FROM pg_stat_statements
            ORDER BY total_time DESC
            LIMIT 10
          metrics:
            total_time:
              metric_type: gauge
            calls:
              metric_type: rate
            rows:
              metric_type: rate

        - query: |
            SELECT count(*) as active_connections
            FROM pg_stat_activity
            WHERE state = 'active'
          metrics:
            active_connections:
              metric_type: gauge

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\k6\Dockerfile ######
FROM grafana/k6:latest

COPY scripts/load-test.js /scripts/load-test.js

ENTRYPOINT ["k6", "run"]
CMD ["/scripts/load-test.js"]

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\k6\scripts\load-test.js ######
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend } from 'k6/metrics';

const errorRate = new Rate('errors');
const queryDuration = new Trend('query_duration');

// New Relic configuration
const NR_LICENSE_KEY = __ENV.NEW_RELIC_LICENSE_KEY;
const NR_APP_NAME = __ENV.NEW_RELIC_APP_NAME || 'postgres-perf-demo-load';
const NR_METRICS_API = 'https://metric-api.newrelic.com/metric/v1';

export let options = {
  vus: __ENV.K6_VUS || 10,
  duration: __ENV.K6_DURATION || '5m',
  thresholds: {
    errors: ['rate<0.1'],
    http_req_duration: ['p(95)<2000']
  }
};

function sendMetricsToNewRelic(metrics) {
  const payload = [{
    metrics: [
      {
        name: 'PostgresLoadTest',
        type: 'gauge',
        value: metrics.duration,
        timestamp: Date.now(),
        attributes: {
          queryType: metrics.queryType,
          success: metrics.success,
          statusCode: metrics.statusCode,
          virtualUsers: options.vus
        }
      }
    ]
  }];

  const headers = {
    'Api-Key': NR_LICENSE_KEY,
    'Content-Type': 'application/json'
  };

  http.post(NR_METRICS_API, JSON.stringify(payload), { headers });
}

const ENDPOINTS = [
  { name: 'complex', weight: 0.4 },
  { name: 'lock', weight: 0.3 },
  { name: 'stats', weight: 0.3 }
];

function selectEndpoint() {
  const r = Math.random();
  let sum = 0;
  for (const endpoint of ENDPOINTS) {
    sum += endpoint.weight;
    if (r <= sum) return endpoint.name;
  }
  return ENDPOINTS[0].name;
}

export default function() {
  const queryType = selectEndpoint();
  const url = `http://api:3000/query/${queryType}`;
  
  const startTime = new Date().getTime();
  const response = http.get(url);
  const duration = new Date().getTime() - startTime;
  
  const success = check(response, {
    'status is 200': (r) => r.status === 200
  });

  errorRate.add(!success);
  queryDuration.add(duration);

  sendMetricsToNewRelic({
    queryType,
    duration,
    success,
    statusCode: response.status
  });

  sleep(1);
}

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\scripts\deploy-local.sh ######
#!/bin/bash
set -e

echo "[INFO] Starting deployment process..."

# Check for required files
if [ ! -f .env ]; then
    echo "[ERROR] .env file not found"
    echo "[INFO] Creating from example..."
    cp .env.example .env
    echo "[ACTION] Please edit .env with your settings"
    exit 1
fi

# Create required directories
mkdir -p db-setup/logs

# Check if New Relic license key is set
if ! grep -q "NEW_RELIC_LICENSE_KEY" .env || grep -q "NEW_RELIC_LICENSE_KEY=your_license_key" .env; then
    echo "[ERROR] Please set your New Relic license key in .env file"
    exit 1
fi

echo "[INFO] Stopping any existing containers..."
docker-compose down --remove-orphans

echo "[INFO] Building and starting services..."
docker-compose build
docker-compose up -d

echo "[INFO] Waiting for services to initialize..."
sleep 15

echo "[INFO] Verifying PostgreSQL integration..."
./verify-nri-postgresql.sh

echo "[INFO] Checking container status..."
docker-compose ps

echo "[INFO] Deployment complete!"
echo "[INFO] Access the API at: http://localhost:${API_PORT:-3000}"
echo "[INFO] Monitor the logs with: docker-compose logs -f"

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\scripts\reset-local.sh ######
#!/bin/bash
set -e

echo "[INFO] Starting environment reset..."

# Stop all containers
echo "[INFO] Stopping all containers..."
docker-compose down

# Remove volumes
echo "[INFO] Removing volumes..."
docker volume rm $(docker volume ls -q | grep postgres-perf) 2>/dev/null || true

# Clean up Docker resources
echo "[INFO] Cleaning up Docker resources..."
docker system prune -f

echo "[INFO] Environment reset complete!"
echo "[INFO] To redeploy, run: make deploy"

###### File: C:\Users\hi\Desktop\SourceCode\db-perf-env\psql\scripts\verify-nri-postgresql.sh ######
#!/bin/bash
set -e

echo "Verifying PostgreSQL New Relic integration..."

# Check if the integration binary exists
echo "Checking integration binary..."
docker-compose exec newrelic-infra-bundle ls -l /var/db/newrelic-infra/newrelic-integrations/bin/nri-postgresql

# Check the version
echo "Checking integration version..."
docker-compose exec newrelic-infra-bundle /var/db/newrelic-infra/newrelic-integrations/bin/nri-postgresql -version

# Test the integration
echo "Testing integration connection..."
docker-compose exec newrelic-infra-bundle /var/db/newrelic-infra/newrelic-integrations/bin/nri-postgresql \
  -hostname postgres-perf \
  -port 5432 \
  -username ${POSTGRES_MONITOR_USER} \
  -password ${POSTGRES_MONITOR_PASSWORD} \
  -database ${POSTGRES_DB}

echo "Verification complete!"

